{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open VSCode and create a new project folder for your machine learning project.\n",
    "# Open the terminal in VSCode by going to Terminal > New Terminal.\n",
    "# Create a new environment using conda or pip. For example, to create a new environment with conda:\n",
    "#     conda create --name myenv\n",
    "#     conda activate myenv\n",
    "# Install the necessary packages for your machine learning project. For example, to install scikit-learn:\n",
    "#     conda install pandas numpy scikit-learn flask\n",
    "#     pip install -r requirements.txt             use this if you had initially loaded some packages to the file\n",
    "# Export the dependencies of your project by running the command:\n",
    "#     pip freeze > requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Table of Content:\n",
    "* [Import Libraries](#1)\n",
    "* [Load Data](#2)\n",
    "* [Exploratory Data Analysis (EDA)](#3)\n",
    "* [Data Preprocessing](#4)\n",
    "    * [Data Cleaning](#4a)\n",
    "    * [Data Transformation](#4b)\n",
    "    * [Handling Imbalanced Data](#4c)\n",
    "    * [Data Reduction](#4d)\n",
    "* [Selecting and Training the Model](#5) \n",
    "* [Model Evaluation](#6) \n",
    "* [Feature Engineering](#7) \n",
    "* [Model Optimization](#8) \n",
    "* [Model Deployment](#9) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis      \n",
    "import pandas as pd          # data analysis library for handling structured data             \n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "from metrics import *\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "import missingno as msno    #visualize missing data\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 50) \n",
    "pd.set_option('display.max_columns', 500) \n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiment_1 = pd.read_csv('exp1', index_col='time', parse_dates=True)\n",
    "experiment_2 = pd.read_csv('exp2', index_col='time', parse_dates=True)\n",
    "# Repeat for all experiments...\n",
    "experiment_10 = pd.read_csv('exp10', index_col='time', parse_dates=True)\n",
    "\n",
    "# Combine all experiments into a list for easy processing\n",
    "experiments = [experiment_1, experiment_2, ..., experiment_10]\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT DATAFRAMES TO PYTORCH \n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    # Assuming 'df' is preprocessed and ready for conversion\n",
    "    # Convert df values to tensor\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# Convert each experiment to a tensor and store in a new list\n",
    "tensor_experiments = [df_to_tensor(df) for df in experiments]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPARE DATALOADERS\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Combine original features with extra parameters (one approach)\n",
    "combined_data = np.concatenate([tensor_experiments, extra_params], axis=-1)  # New shape [n_experiments, time_steps, n_features + n_extra_features]\n",
    "\n",
    "# Example targets for demonstration\n",
    "# In a real scenario, replace 'dummy_targets' with your actual targets\n",
    "dummy_targets = torch.rand(len(tensor_experiments), output_dim)  # Randomly generated targets\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(tensor_experiments), dummy_targets)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BUILDING THE MODEL\n",
    "\n",
    "\n",
    "    # Tips \n",
    "        #1. Consider Multi-Task Learning (MTL):\n",
    "            # Multi-Task Learning is a learning paradigm in machine learning where multiple learning tasks are solved at the same time, \n",
    "            # while exploiting commonalities and differences across tasks. This approach can lead to improved learning efficiency and \n",
    "            # prediction accuracy for the task models, especially when the tasks are related. In deep learning, this often involves \n",
    "            # sharing layers between tasks, while having some task-specific layers towards the end of the mode \n",
    "            \n",
    "            # Imagine you have a dataset from your experiments, and you want to predict 4 main outputs, but you also have 3 other \n",
    "            # parameters that are somewhat related and you believe predicting them could help improve the performance of your main task.\n",
    "            \n",
    "            # In this example, MultiTaskLSTM is designed to make predictions for both the main task and an auxiliary task using shared \n",
    "            # LSTM layers for feature extraction, and separate fully connected layers for each task's specific output.\n",
    "            \n",
    "            \n",
    "        # 2. Consider Attention Mechanisms (Best to use Transformer ones)\n",
    "            # Attention mechanisms allow models to focus on different parts of the input for each output part, improving the ability of \n",
    "            # the model to capture dependencies, especially in sequences. In deep learning, attention mechanisms can dynamically weight \n",
    "            # the importance of input elements.\n",
    "            \n",
    "            # example: In this AttentionModel, an attention mechanism is applied to the output of an LSTM layer. The model computes \n",
    "            # attention weights for each element in the sequence, which are then used to create a weighted sum (context vector) \n",
    "            # representing the input sequence. This context vector is then fed into a fully connected layer to produce the final output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vessel Type</th>\n",
       "      <th>Vessel Volume</th>\n",
       "      <th>Vessel Name</th>\n",
       "      <th>Production day</th>\n",
       "      <th>Timepoint (hr)</th>\n",
       "      <th>Agitation</th>\n",
       "      <th>DO</th>\n",
       "      <th>pH setpoint</th>\n",
       "      <th>Gas flow</th>\n",
       "      <th>Air (%)</th>\n",
       "      <th>O2</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Media type</th>\n",
       "      <th>Feed Type</th>\n",
       "      <th>Glucose Limit</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Timepoint (hr).1</th>\n",
       "      <th>OD600</th>\n",
       "      <th>WCW (g/L)</th>\n",
       "      <th>Agitation.1</th>\n",
       "      <th>Air %</th>\n",
       "      <th>D0 %</th>\n",
       "      <th>GasFlow</th>\n",
       "      <th>O2.1</th>\n",
       "      <th>Ph</th>\n",
       "      <th>Feed %</th>\n",
       "      <th>Feed</th>\n",
       "      <th>Temp.1</th>\n",
       "      <th>Titre (mg/ml)</th>\n",
       "      <th>Glycerol (g/L)</th>\n",
       "      <th>Glucose (g/L)</th>\n",
       "      <th>Acetate (mmol/L)</th>\n",
       "      <th>Phosphate (mmol/L)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1185.371948</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.209160</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.778956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32150</td>\n",
       "      <td>1.65</td>\n",
       "      <td>26.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.501</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1184.802002</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>82.209839</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.804696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1185.295044</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.353783</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.850261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>7.74</td>\n",
       "      <td>20.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1185.056030</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.379463</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.802261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.560</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1184.803955</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>57.830719</td>\n",
       "      <td>5.000183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.715652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>24.82</td>\n",
       "      <td>19.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>104.600</td>\n",
       "      <td>224.5</td>\n",
       "      <td>1185.343994</td>\n",
       "      <td>91.940117</td>\n",
       "      <td>40.277081</td>\n",
       "      <td>4.996964</td>\n",
       "      <td>8.056145</td>\n",
       "      <td>6.724696</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.27313</td>\n",
       "      <td>1.75</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>108.800</td>\n",
       "      <td>212.0</td>\n",
       "      <td>1185.397949</td>\n",
       "      <td>93.899193</td>\n",
       "      <td>40.614971</td>\n",
       "      <td>4.960910</td>\n",
       "      <td>6.100883</td>\n",
       "      <td>6.710783</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.74359</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>111.000</td>\n",
       "      <td>205.0</td>\n",
       "      <td>1184.901001</td>\n",
       "      <td>94.543854</td>\n",
       "      <td>40.665649</td>\n",
       "      <td>4.996494</td>\n",
       "      <td>5.455448</td>\n",
       "      <td>6.730957</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.12183</td>\n",
       "      <td>1.43</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>111.000</td>\n",
       "      <td>211.5</td>\n",
       "      <td>1185.083984</td>\n",
       "      <td>95.415733</td>\n",
       "      <td>39.871601</td>\n",
       "      <td>4.979597</td>\n",
       "      <td>4.631737</td>\n",
       "      <td>6.712174</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>29.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.97287</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm1</td>\n",
       "      <td>2</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1185</td>\n",
       "      <td>100</td>\n",
       "      <td>6.8</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>111.800</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1185.359009</td>\n",
       "      <td>96.305038</td>\n",
       "      <td>39.601292</td>\n",
       "      <td>4.919702</td>\n",
       "      <td>3.693306</td>\n",
       "      <td>6.703130</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.425</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.17062</td>\n",
       "      <td>1.67</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Vessel Type Vessel Volume Vessel Name  Production day  Timepoint (hr)  Agitation   DO  pH setpoint  Gas flow  Air (%)  O2  Temp Media type Feed Type  Glucose Limit  Unnamed: 15  Timepoint (hr).1    OD600  WCW (g/L)  Agitation.1       Air %       D0 %   GasFlow      O2.1        Ph  Feed %   Feed  Temp.1  Titre (mg/ml)  Glycerol (g/L)  Glucose (g/L)  Acetate (mmol/L)  Phosphate (mmol/L)\n",
       "0   5L Fermenter        5000ml       Ferm1               1             0.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN               0.0    0.305        NaN  1185.371948  100.000000  98.209160  5.000001  0.000000  6.778956     0.0  0.000   30.03            NaN             NaN        0.32150              1.65               26.21\n",
       "1   5L Fermenter        5000ml       Ferm1               1             2.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN               2.0    0.501        NaN  1184.802002  100.000000  82.209839  5.000001  0.000000  6.804696     0.0  0.000   30.02            NaN             NaN            NaN               NaN                 NaN\n",
       "2   5L Fermenter        5000ml       Ferm1               1             4.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN               4.0    1.550        NaN  1185.295044  100.000000  74.353783  5.000001  0.000000  6.850261     0.0  0.000   30.01            NaN             NaN        0.00000              7.74               20.01\n",
       "3   5L Fermenter        5000ml       Ferm1               1             6.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN               6.0    4.590        NaN  1185.056030  100.000000  66.379463  5.000001  0.000000  6.802261     0.0  0.000   30.00            NaN             NaN            NaN               NaN                 NaN\n",
       "4   5L Fermenter        5000ml       Ferm1               1             8.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN               8.0   11.560        NaN  1184.803955  100.000000  57.830719  5.000183  0.000000  6.715652     0.0  0.000   30.00            NaN             NaN        0.00000             24.82               19.55\n",
       "..           ...           ...         ...             ...             ...        ...  ...          ...       ...      ...  ..   ...        ...       ...            ...          ...               ...      ...        ...          ...         ...        ...       ...       ...       ...     ...    ...     ...            ...             ...            ...               ...                 ...\n",
       "16  5L Fermenter        5000ml       Ferm1               2            32.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN              32.0  104.600      224.5  1185.343994   91.940117  40.277081  4.996964  8.056145  6.724696    12.5  0.425   30.00            NaN             NaN        9.27313              1.75                0.00\n",
       "17  5L Fermenter        5000ml       Ferm1               2            36.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN              36.0  108.800      212.0  1185.397949   93.899193  40.614971  4.960910  6.100883  6.710783    12.5  0.425   30.00            NaN             NaN        8.74359              1.38                0.00\n",
       "18  5L Fermenter        5000ml       Ferm1               2            40.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN              40.0  111.000      205.0  1184.901001   94.543854  40.665649  4.996494  5.455448  6.730957    12.5  0.425   30.00            NaN             NaN        9.12183              1.43                0.00\n",
       "19  5L Fermenter        5000ml       Ferm1               2            44.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN              44.0  111.000      211.5  1185.083984   95.415733  39.871601  4.979597  4.631737  6.712174    12.5  0.425   29.72            NaN             NaN        9.97287              1.50                0.00\n",
       "20  5L Fermenter        5000ml       Ferm1               2            48.0       1185  100          6.8         5      100   0    30    Media A   Glucose              0          NaN              48.0  111.800      199.0  1185.359009   96.305038  39.601292  4.919702  3.693306  6.703130    12.5  0.425   30.00            NaN             NaN       11.17062              1.67                0.00\n",
       "\n",
       "[21 rows x 33 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_url = r\"C:\\Users\\pault\\OneDrive - University of Oklahoma\\GRA - Bio-Manufacturing\\1. ML-Cytovance-OU-Research\\data\\raw\\ProjectG_Data.xlsx\"\n",
    "\n",
    "\n",
    "df_210623 = pd.read_excel(file_url, sheet_name=0, header=2)\n",
    "exp_210623_1 = df_210623.iloc[:22,]\n",
    "exp_210623_2 = df_210623.iloc[22:44,]\n",
    "exp_210623_3 = df_210623.iloc[44:66,]\n",
    "exp_210623_4 = df_210623.iloc[66:88,]\n",
    "\n",
    "df_211130 = pd.read_excel(file_url, sheet_name=1, header=2)\n",
    "exp_211130_1 = df_211130.iloc[:21,]\n",
    "exp_211130_2 = df_211130.iloc[21:42,]\n",
    "exp_211130_3 = df_211130.iloc[42:63,]\n",
    "exp_211130_4 = df_211130.iloc[63:84,] \n",
    "\n",
    "df_211013 = pd.read_excel(file_url, sheet_name=2, header=2)\n",
    "exp_211013_1 = df_211013.iloc[:19,]\n",
    "exp_211013_2 = df_211013.iloc[19:38,]\n",
    "exp_211013_3 = df_211013.iloc[38:57,]\n",
    "exp_211013_4 = df_211013.iloc[57:76]\n",
    "\n",
    "df_220822 = pd.read_excel(file_url, sheet_name=3, header=2)\n",
    "exp_220822_1 = df_220822.iloc[:20,]\n",
    "exp_220822_2 = df_220822.iloc[20:40,]\n",
    "exp_220822_3 = df_220822.iloc[40:60,]\n",
    "exp_220822_4 = df_220822.iloc[60:80,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 2)) + list(range(3, 24)) + list(range(44, 101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vessel Type</th>\n",
       "      <th>Vessel Volume</th>\n",
       "      <th>Vessel Name</th>\n",
       "      <th>Production day</th>\n",
       "      <th>Timepoint (hr)</th>\n",
       "      <th>Agitation</th>\n",
       "      <th>DO</th>\n",
       "      <th>pH setpoint</th>\n",
       "      <th>Gas flow</th>\n",
       "      <th>Air (%)</th>\n",
       "      <th>O2</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Media type</th>\n",
       "      <th>Feed Type</th>\n",
       "      <th>Glucose Limit</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Timepoint (hr).1</th>\n",
       "      <th>OD600</th>\n",
       "      <th>WCW (g/L)</th>\n",
       "      <th>Agitation.1</th>\n",
       "      <th>Air %</th>\n",
       "      <th>D0 %</th>\n",
       "      <th>GasFlow</th>\n",
       "      <th>O2.1</th>\n",
       "      <th>Ph</th>\n",
       "      <th>Feed %</th>\n",
       "      <th>Feed</th>\n",
       "      <th>Temp.1</th>\n",
       "      <th>Titre (mg/ml)</th>\n",
       "      <th>Glycerol (g/L)</th>\n",
       "      <th>Glucose (g/L)</th>\n",
       "      <th>Acetate (mmol/L)</th>\n",
       "      <th>Phosphate (mmol/L)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1184.807007</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.149399</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.341572</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.378667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1184.873047</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>98.996590</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.341572</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>29.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.779667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1185.260010</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>96.939941</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.383945</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.323333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1184.755981</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>86.891548</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.386256</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1184.802002</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>71.833191</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.397812</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.566667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1185.883057</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>40.670368</td>\n",
       "      <td>4.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.203667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>30.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.21</td>\n",
       "      <td>30.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.5</td>\n",
       "      <td>16.333333</td>\n",
       "      <td>55.5</td>\n",
       "      <td>1185.038940</td>\n",
       "      <td>99.846100</td>\n",
       "      <td>41.453171</td>\n",
       "      <td>4.999999</td>\n",
       "      <td>0.160626</td>\n",
       "      <td>6.361988</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.26180</td>\n",
       "      <td>30.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.16</td>\n",
       "      <td>32.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>82.0</td>\n",
       "      <td>1185.577026</td>\n",
       "      <td>99.423920</td>\n",
       "      <td>37.382580</td>\n",
       "      <td>4.779650</td>\n",
       "      <td>0.568325</td>\n",
       "      <td>6.208290</td>\n",
       "      <td>11.40</td>\n",
       "      <td>0.38760</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.43</td>\n",
       "      <td>29.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.5</td>\n",
       "      <td>46.066667</td>\n",
       "      <td>115.5</td>\n",
       "      <td>1184.692993</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>51.565609</td>\n",
       "      <td>4.999912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.256826</td>\n",
       "      <td>17.18</td>\n",
       "      <td>0.58412</td>\n",
       "      <td>29.95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038170</td>\n",
       "      <td>0.35</td>\n",
       "      <td>21.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.5</td>\n",
       "      <td>50.900000</td>\n",
       "      <td>150.5</td>\n",
       "      <td>1184.899048</td>\n",
       "      <td>92.610481</td>\n",
       "      <td>39.311131</td>\n",
       "      <td>4.683192</td>\n",
       "      <td>7.390408</td>\n",
       "      <td>6.222157</td>\n",
       "      <td>25.60</td>\n",
       "      <td>0.87040</td>\n",
       "      <td>30.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.338080</td>\n",
       "      <td>0.63</td>\n",
       "      <td>17.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.5</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>154.5</td>\n",
       "      <td>1185.041016</td>\n",
       "      <td>93.455338</td>\n",
       "      <td>39.916031</td>\n",
       "      <td>4.589436</td>\n",
       "      <td>6.544758</td>\n",
       "      <td>6.253745</td>\n",
       "      <td>17.90</td>\n",
       "      <td>0.60860</td>\n",
       "      <td>30.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.698090</td>\n",
       "      <td>0.55</td>\n",
       "      <td>15.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.5</td>\n",
       "      <td>62.033333</td>\n",
       "      <td>196.0</td>\n",
       "      <td>1185.208984</td>\n",
       "      <td>84.297012</td>\n",
       "      <td>46.021919</td>\n",
       "      <td>4.442885</td>\n",
       "      <td>15.725340</td>\n",
       "      <td>6.190185</td>\n",
       "      <td>17.90</td>\n",
       "      <td>0.60860</td>\n",
       "      <td>29.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.197520</td>\n",
       "      <td>7.06</td>\n",
       "      <td>5.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.5</td>\n",
       "      <td>72.066667</td>\n",
       "      <td>220.0</td>\n",
       "      <td>1184.712036</td>\n",
       "      <td>86.635071</td>\n",
       "      <td>42.477940</td>\n",
       "      <td>4.457441</td>\n",
       "      <td>13.361880</td>\n",
       "      <td>6.390879</td>\n",
       "      <td>17.90</td>\n",
       "      <td>0.60860</td>\n",
       "      <td>30.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015307</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>212.5</td>\n",
       "      <td>1184.875977</td>\n",
       "      <td>87.380539</td>\n",
       "      <td>41.808990</td>\n",
       "      <td>4.519156</td>\n",
       "      <td>12.617220</td>\n",
       "      <td>6.273775</td>\n",
       "      <td>17.90</td>\n",
       "      <td>0.60860</td>\n",
       "      <td>28.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042020</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>77.800000</td>\n",
       "      <td>207.5</td>\n",
       "      <td>1184.947021</td>\n",
       "      <td>94.142822</td>\n",
       "      <td>40.343010</td>\n",
       "      <td>4.516440</td>\n",
       "      <td>5.857101</td>\n",
       "      <td>6.206364</td>\n",
       "      <td>12.50</td>\n",
       "      <td>0.42500</td>\n",
       "      <td>26.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.860590</td>\n",
       "      <td>2.17</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>83.600000</td>\n",
       "      <td>207.5</td>\n",
       "      <td>1184.912964</td>\n",
       "      <td>99.576492</td>\n",
       "      <td>40.890980</td>\n",
       "      <td>4.793379</td>\n",
       "      <td>0.422386</td>\n",
       "      <td>6.194422</td>\n",
       "      <td>10.60</td>\n",
       "      <td>0.36040</td>\n",
       "      <td>25.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.202700</td>\n",
       "      <td>4.02</td>\n",
       "      <td>-0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "      <td>87.400000</td>\n",
       "      <td>208.5</td>\n",
       "      <td>1184.800049</td>\n",
       "      <td>99.456673</td>\n",
       "      <td>40.179340</td>\n",
       "      <td>4.711772</td>\n",
       "      <td>0.543087</td>\n",
       "      <td>6.226009</td>\n",
       "      <td>9.01</td>\n",
       "      <td>0.30634</td>\n",
       "      <td>26.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.399210</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "      <td>93.266667</td>\n",
       "      <td>217.5</td>\n",
       "      <td>1184.574951</td>\n",
       "      <td>99.654694</td>\n",
       "      <td>40.592091</td>\n",
       "      <td>4.850256</td>\n",
       "      <td>0.344494</td>\n",
       "      <td>6.214839</td>\n",
       "      <td>7.66</td>\n",
       "      <td>0.26044</td>\n",
       "      <td>26.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.133740</td>\n",
       "      <td>4.10</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>93.400000</td>\n",
       "      <td>216.5</td>\n",
       "      <td>1185.463989</td>\n",
       "      <td>99.543571</td>\n",
       "      <td>37.069462</td>\n",
       "      <td>4.800520</td>\n",
       "      <td>0.463291</td>\n",
       "      <td>6.221002</td>\n",
       "      <td>6.32</td>\n",
       "      <td>0.21488</td>\n",
       "      <td>25.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.673240</td>\n",
       "      <td>4.84</td>\n",
       "      <td>-0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5L Fermenter</td>\n",
       "      <td>5000ml</td>\n",
       "      <td>Ferm4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Media A</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.0</td>\n",
       "      <td>86.933333</td>\n",
       "      <td>220.5</td>\n",
       "      <td>1185.413940</td>\n",
       "      <td>99.595612</td>\n",
       "      <td>38.563911</td>\n",
       "      <td>4.996673</td>\n",
       "      <td>0.405830</td>\n",
       "      <td>6.222928</td>\n",
       "      <td>5.37</td>\n",
       "      <td>0.18258</td>\n",
       "      <td>25.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.957560</td>\n",
       "      <td>4.10</td>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Vessel Type Vessel Volume Vessel Name  Production day  Timepoint (hr)  Agitation     DO  pH setpoint  Gas flow  Air (%)   O2  Temp Media type Feed Type  Glucose Limit  Unnamed: 15  Timepoint (hr).1      OD600  WCW (g/L)  Agitation.1       Air %        D0 %   GasFlow       O2.1        Ph  Feed %     Feed  Temp.1  Titre (mg/ml)  Glycerol (g/L)  Glucose (g/L)  Acetate (mmol/L)  Phosphate (mmol/L)\n",
       "60  5L Fermenter        5000ml       Ferm4             1.0             0.0     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN               0.0   0.378667        NaN  1184.807007  100.000000  100.149399  5.000001   0.000000  6.341572    0.00  0.00000   30.01            NaN             NaN            NaN               NaN                 NaN\n",
       "61  5L Fermenter        5000ml       Ferm4             1.0             2.0     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN               2.0   0.378667        NaN  1184.873047  100.000000   98.996590  5.000001   0.000000  6.341572    0.00  0.00000   29.99            NaN             NaN            NaN               NaN                 NaN\n",
       "62  5L Fermenter        5000ml       Ferm4             1.0             4.0     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN               4.0   0.779667        NaN  1185.260010  100.000000   96.939941  5.000001   0.000000  6.383945    0.00  0.00000   30.03            NaN             NaN            NaN               NaN                 NaN\n",
       "63  5L Fermenter        5000ml       Ferm4             1.0             6.0     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN               6.0   2.323333        NaN  1184.755981  100.000000   86.891548  5.000001   0.000000  6.386256    0.00  0.00000   30.03            NaN             NaN            NaN               NaN                 NaN\n",
       "64  5L Fermenter        5000ml       Ferm4             1.0             8.0     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN               8.0   5.950000        NaN  1184.802002  100.000000   71.833191  5.000001   0.000000  6.397812    0.00  0.00000   30.00            NaN             NaN            NaN               NaN                 NaN\n",
       "65  5L Fermenter        5000ml       Ferm4             1.0            10.0     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              10.0  12.566667        NaN  1185.883057  100.000000   40.670368  4.999999   0.000000  6.203667    0.00  0.00000   30.02            NaN            2.07            NaN             17.21               30.94\n",
       "66  5L Fermenter        5000ml       Ferm4             1.0            10.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              10.5  16.333333       55.5  1185.038940   99.846100   41.453171  4.999999   0.160626  6.361988    7.70  0.26180   30.01            NaN            0.00            NaN             11.16               32.07\n",
       "67  5L Fermenter        5000ml       Ferm4             1.0            12.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              12.5  28.900000       82.0  1185.577026   99.423920   37.382580  4.779650   0.568325  6.208290   11.40  0.38760   30.00            NaN            0.00            NaN              0.43               29.10\n",
       "68  5L Fermenter        5000ml       Ferm4             1.0            14.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              14.5  46.066667      115.5  1184.692993  100.000000   51.565609  4.999912   0.000000  6.256826   17.18  0.58412   29.95            NaN             NaN      -0.038170              0.35               21.25\n",
       "69  5L Fermenter        5000ml       Ferm4             1.0            16.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              16.5  50.900000      150.5  1184.899048   92.610481   39.311131  4.683192   7.390408  6.222157   25.60  0.87040   30.01            NaN             NaN       5.338080              0.63               17.66\n",
       "70  5L Fermenter        5000ml       Ferm4             1.0            18.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              18.5  52.000000      154.5  1185.041016   93.455338   39.916031  4.589436   6.544758  6.253745   17.90  0.60860   30.00            NaN             NaN      12.698090              0.55               15.60\n",
       "71  5L Fermenter        5000ml       Ferm4             1.0            20.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              20.5  62.033333      196.0  1185.208984   84.297012   46.021919  4.442885  15.725340  6.190185   17.90  0.60860   29.96            NaN             NaN       4.197520              7.06                5.46\n",
       "72  5L Fermenter        5000ml       Ferm4             1.0            22.5     1185.0  100.0          6.3       5.0    100.0  0.0  30.0    Media A   Glucose            0.0          NaN              22.5  72.066667      220.0  1184.712036   86.635071   42.477940  4.457441  13.361880  6.390879   17.90  0.60860   30.01            NaN             NaN       0.015307              6.95                0.42\n",
       "73  5L Fermenter        5000ml       Ferm4             2.0            24.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              24.0  77.000000      212.5  1184.875977   87.380539   41.808990  4.519156  12.617220  6.273775   17.90  0.60860   28.70            NaN             NaN      -0.042020              2.63                0.03\n",
       "74  5L Fermenter        5000ml       Ferm4             2.0            28.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              28.0  77.800000      207.5  1184.947021   94.142822   40.343010  4.516440   5.857101  6.206364   12.50  0.42500   26.01            NaN             NaN      15.860590              2.17               -0.01\n",
       "75  5L Fermenter        5000ml       Ferm4             2.0            32.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              32.0  83.600000      207.5  1184.912964   99.576492   40.890980  4.793379   0.422386  6.194422   10.60  0.36040   25.99            NaN             NaN      19.202700              4.02               -0.01\n",
       "76  5L Fermenter        5000ml       Ferm4             2.0            36.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              36.0  87.400000      208.5  1184.800049   99.456673   40.179340  4.711772   0.543087  6.226009    9.01  0.30634   26.00            NaN             NaN      18.399210              2.78               -0.02\n",
       "77  5L Fermenter        5000ml       Ferm4             2.0            40.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              40.0  93.266667      217.5  1184.574951   99.654694   40.592091  4.850256   0.344494  6.214839    7.66  0.26044   26.01            NaN             NaN      16.133740              4.10               -0.03\n",
       "78  5L Fermenter        5000ml       Ferm4             2.0            44.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              44.0  93.400000      216.5  1185.463989   99.543571   37.069462  4.800520   0.463291  6.221002    6.32  0.21488   25.98            NaN             NaN      11.673240              4.84               -0.11\n",
       "79  5L Fermenter        5000ml       Ferm4             2.0            48.0     1185.0  100.0          6.3       5.0    100.0  0.0  26.0    Media A   Glucose            0.0          NaN              48.0  86.933333      220.5  1185.413940   99.595612   38.563911  4.996673   0.405830  6.222928    5.37  0.18258   25.99            NaN             NaN       7.957560              4.10               -0.09"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_220822 = pd.read_excel(file_url, sheet_name=3, header=2)\n",
    "\n",
    "exp_220822_1 = df_220822.iloc[:20,]\n",
    "exp_220822_2 = df_220822.iloc[20:40,]\n",
    "exp_220822_3 = df_220822.iloc[40:60,]\n",
    "exp_220822_4 = df_220822.iloc[60:80,]\n",
    "\n",
    "\n",
    "exp_220822_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0msheet_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | int | list[IntStrT] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mheader\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | Sequence[int] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list[str] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mindex_col\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | Sequence[int] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0musecols\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | str | Sequence[int] | Sequence[str] | Callable[[str], bool] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DtypeArg | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Literal['xlrd', 'openpyxl', 'odf', 'pyxlsb'] | None\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mconverters\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'dict[str, Callable] | dict[int, Callable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mtrue_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Iterable[Hashable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfalse_values\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Iterable[Hashable] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mskiprows\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Sequence[int] | int | Callable[[int], object] | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mnrows\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mna_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mkeep_default_na\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mna_filter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparse_dates\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'list | dict | bool'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdate_parser\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'Callable | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdate_format\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'dict[Hashable, str] | str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthousands\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdecimal\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcomment\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'str | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mskipfooter\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'int'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mstorage_options\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'StorageOptions | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdtype_backend\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'DtypeBackend | lib.NoDefault'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m<\u001b[0m\u001b[0mno_default\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mengine_kwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'dict | None'\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m'DataFrame | dict[IntStrT, DataFrame]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Read an Excel file into a pandas DataFrame.\n",
      "\n",
      "Supports `xls`, `xlsx`, `xlsm`, `xlsb`, `odf`, `ods` and `odt` file extensions\n",
      "read from a local filesystem or URL. Supports an option to read\n",
      "a single sheet or a list of sheets.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "io : str, bytes, ExcelFile, xlrd.Book, path object, or file-like object\n",
      "    Any valid string path is acceptable. The string could be a URL. Valid\n",
      "    URL schemes include http, ftp, s3, and file. For file URLs, a host is\n",
      "    expected. A local file could be: ``file://localhost/path/to/table.xlsx``.\n",
      "\n",
      "    If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "\n",
      "    By file-like object, we refer to objects with a ``read()`` method,\n",
      "    such as a file handle (e.g. via builtin ``open`` function)\n",
      "    or ``StringIO``.\n",
      "\n",
      "    .. deprecated:: 2.1.0\n",
      "        Passing byte strings is deprecated. To read from a\n",
      "        byte string, wrap it in a ``BytesIO`` object.\n",
      "sheet_name : str, int, list, or None, default 0\n",
      "    Strings are used for sheet names. Integers are used in zero-indexed\n",
      "    sheet positions (chart sheets do not count as a sheet position).\n",
      "    Lists of strings/integers are used to request multiple sheets.\n",
      "    Specify None to get all worksheets.\n",
      "\n",
      "    Available cases:\n",
      "\n",
      "    * Defaults to ``0``: 1st sheet as a `DataFrame`\n",
      "    * ``1``: 2nd sheet as a `DataFrame`\n",
      "    * ``\"Sheet1\"``: Load sheet with name \"Sheet1\"\n",
      "    * ``[0, 1, \"Sheet5\"]``: Load first, second and sheet named \"Sheet5\"\n",
      "      as a dict of `DataFrame`\n",
      "    * None: All worksheets.\n",
      "\n",
      "header : int, list of int, default 0\n",
      "    Row (0-indexed) to use for the column labels of the parsed\n",
      "    DataFrame. If a list of integers is passed those row positions will\n",
      "    be combined into a ``MultiIndex``. Use None if there is no header.\n",
      "names : array-like, default None\n",
      "    List of column names to use. If file contains no header row,\n",
      "    then you should explicitly pass header=None.\n",
      "index_col : int, str, list of int, default None\n",
      "    Column (0-indexed) to use as the row labels of the DataFrame.\n",
      "    Pass None if there is no such column.  If a list is passed,\n",
      "    those columns will be combined into a ``MultiIndex``.  If a\n",
      "    subset of data is selected with ``usecols``, index_col\n",
      "    is based on the subset.\n",
      "\n",
      "    Missing values will be forward filled to allow roundtripping with\n",
      "    ``to_excel`` for ``merged_cells=True``. To avoid forward filling the\n",
      "    missing values use ``set_index`` after reading the data instead of\n",
      "    ``index_col``.\n",
      "usecols : str, list-like, or callable, default None\n",
      "    * If None, then parse all columns.\n",
      "    * If str, then indicates comma separated list of Excel column letters\n",
      "      and column ranges (e.g. \"A:E\" or \"A,C,E:F\"). Ranges are inclusive of\n",
      "      both sides.\n",
      "    * If list of int, then indicates list of column numbers to be parsed\n",
      "      (0-indexed).\n",
      "    * If list of string, then indicates list of column names to be parsed.\n",
      "    * If callable, then evaluate each column name against it and parse the\n",
      "      column if the callable returns ``True``.\n",
      "\n",
      "    Returns a subset of the columns according to behavior above.\n",
      "dtype : Type name or dict of column -> type, default None\n",
      "    Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32}\n",
      "    Use `object` to preserve data as stored in Excel and not interpret dtype.\n",
      "    If converters are specified, they will be applied INSTEAD\n",
      "    of dtype conversion.\n",
      "engine : str, default None\n",
      "    If io is not a buffer or path, this must be set to identify io.\n",
      "    Supported engines: \"xlrd\", \"openpyxl\", \"odf\", \"pyxlsb\".\n",
      "    Engine compatibility :\n",
      "\n",
      "    - \"xlrd\" supports old-style Excel files (.xls).\n",
      "    - \"openpyxl\" supports newer Excel file formats.\n",
      "    - \"odf\" supports OpenDocument file formats (.odf, .ods, .odt).\n",
      "    - \"pyxlsb\" supports Binary Excel files.\n",
      "\n",
      "    .. versionchanged:: 1.2.0\n",
      "        The engine `xlrd <https://xlrd.readthedocs.io/en/latest/>`_\n",
      "        now only supports old-style ``.xls`` files.\n",
      "        When ``engine=None``, the following logic will be\n",
      "        used to determine the engine:\n",
      "\n",
      "       - If ``path_or_buffer`` is an OpenDocument format (.odf, .ods, .odt),\n",
      "         then `odf <https://pypi.org/project/odfpy/>`_ will be used.\n",
      "       - Otherwise if ``path_or_buffer`` is an xls format,\n",
      "         ``xlrd`` will be used.\n",
      "       - Otherwise if ``path_or_buffer`` is in xlsb format,\n",
      "         ``pyxlsb`` will be used.\n",
      "\n",
      "         .. versionadded:: 1.3.0\n",
      "       - Otherwise ``openpyxl`` will be used.\n",
      "\n",
      "         .. versionchanged:: 1.3.0\n",
      "\n",
      "converters : dict, default None\n",
      "    Dict of functions for converting values in certain columns. Keys can\n",
      "    either be integers or column labels, values are functions that take one\n",
      "    input argument, the Excel cell content, and return the transformed\n",
      "    content.\n",
      "true_values : list, default None\n",
      "    Values to consider as True.\n",
      "false_values : list, default None\n",
      "    Values to consider as False.\n",
      "skiprows : list-like, int, or callable, optional\n",
      "    Line numbers to skip (0-indexed) or number of lines to skip (int) at the\n",
      "    start of the file. If callable, the callable function will be evaluated\n",
      "    against the row indices, returning True if the row should be skipped and\n",
      "    False otherwise. An example of a valid callable argument would be ``lambda\n",
      "    x: x in [0, 2]``.\n",
      "nrows : int, default None\n",
      "    Number of rows to parse.\n",
      "na_values : scalar, str, list-like, or dict, default None\n",
      "    Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "    per-column NA values. By default the following values are interpreted\n",
      "    as NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "    '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'None',\n",
      "    'n/a', 'nan', 'null'.\n",
      "keep_default_na : bool, default True\n",
      "    Whether or not to include the default NaN values when parsing the data.\n",
      "    Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "\n",
      "    * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "      is appended to the default NaN values used for parsing.\n",
      "    * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "      the default NaN values are used for parsing.\n",
      "    * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "      the NaN values specified `na_values` are used for parsing.\n",
      "    * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "      strings will be parsed as NaN.\n",
      "\n",
      "    Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "    `na_values` parameters will be ignored.\n",
      "na_filter : bool, default True\n",
      "    Detect missing value markers (empty strings and the value of na_values). In\n",
      "    data without any NAs, passing na_filter=False can improve the performance\n",
      "    of reading a large file.\n",
      "verbose : bool, default False\n",
      "    Indicate number of NA values placed in non-numeric columns.\n",
      "parse_dates : bool, list-like, or dict, default False\n",
      "    The behavior is as follows:\n",
      "\n",
      "    * bool. If True -> try parsing the index.\n",
      "    * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "      each as a separate date column.\n",
      "    * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "      a single date column.\n",
      "    * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "      result 'foo'\n",
      "\n",
      "    If a column or index contains an unparsable date, the entire column or\n",
      "    index will be returned unaltered as an object data type. If you don`t want to\n",
      "    parse some cells as date just change their type in Excel to \"Text\".\n",
      "    For non-standard datetime parsing, use ``pd.to_datetime`` after ``pd.read_excel``.\n",
      "\n",
      "    Note: A fast-path exists for iso8601-formatted dates.\n",
      "date_parser : function, optional\n",
      "    Function to use for converting a sequence of string columns to an array of\n",
      "    datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "    conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "    advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "    (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "    string values from the columns defined by `parse_dates` into a single array\n",
      "    and pass that; and 3) call `date_parser` once for each row using one or\n",
      "    more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "    arguments.\n",
      "\n",
      "    .. deprecated:: 2.0.0\n",
      "       Use ``date_format`` instead, or read in as ``object`` and then apply\n",
      "       :func:`to_datetime` as-needed.\n",
      "date_format : str or dict of column -> format, default ``None``\n",
      "   If used in conjunction with ``parse_dates``, will parse dates according to this\n",
      "   format. For anything more complex,\n",
      "   please read in as ``object`` and then apply :func:`to_datetime` as-needed.\n",
      "\n",
      "   .. versionadded:: 2.0.0\n",
      "thousands : str, default None\n",
      "    Thousands separator for parsing string columns to numeric.  Note that\n",
      "    this parameter is only necessary for columns stored as TEXT in Excel,\n",
      "    any numeric columns will automatically be parsed, regardless of display\n",
      "    format.\n",
      "decimal : str, default '.'\n",
      "    Character to recognize as decimal point for parsing string columns to numeric.\n",
      "    Note that this parameter is only necessary for columns stored as TEXT in Excel,\n",
      "    any numeric columns will automatically be parsed, regardless of display\n",
      "    format.(e.g. use ',' for European data).\n",
      "\n",
      "    .. versionadded:: 1.4.0\n",
      "\n",
      "comment : str, default None\n",
      "    Comments out remainder of line. Pass a character or characters to this\n",
      "    argument to indicate comments in the input file. Any data between the\n",
      "    comment string and the end of the current line is ignored.\n",
      "skipfooter : int, default 0\n",
      "    Rows at the end to skip (0-indexed).\n",
      "storage_options : dict, optional\n",
      "    Extra options that make sense for a particular storage connection, e.g.\n",
      "    host, port, username, password, etc. For HTTP(S) URLs the key-value pairs\n",
      "    are forwarded to ``urllib.request.Request`` as header options. For other\n",
      "    URLs (e.g. starting with \"s3://\", and \"gcs://\") the key-value pairs are\n",
      "    forwarded to ``fsspec.open``. Please see ``fsspec`` and ``urllib`` for more\n",
      "    details, and for more examples on storage options refer `here\n",
      "    <https://pandas.pydata.org/docs/user_guide/io.html?\n",
      "    highlight=storage_options#reading-writing-remote-files>`_.\n",
      "\n",
      "    .. versionadded:: 1.2.0\n",
      "\n",
      "dtype_backend : {'numpy_nullable', 'pyarrow'}, default 'numpy_nullable'\n",
      "    Back-end data type applied to the resultant :class:`DataFrame`\n",
      "    (still experimental). Behaviour is as follows:\n",
      "\n",
      "    * ``\"numpy_nullable\"``: returns nullable-dtype-backed :class:`DataFrame`\n",
      "      (default).\n",
      "    * ``\"pyarrow\"``: returns pyarrow-backed nullable :class:`ArrowDtype`\n",
      "      DataFrame.\n",
      "\n",
      "    .. versionadded:: 2.0\n",
      "\n",
      "engine_kwargs : dict, optional\n",
      "    Arbitrary keyword arguments passed to excel engine.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "DataFrame or dict of DataFrames\n",
      "    DataFrame from the passed in Excel file. See notes in sheet_name\n",
      "    argument for more information on when a dict of DataFrames is returned.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DataFrame.to_excel : Write DataFrame to an Excel file.\n",
      "DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "For specific information on the methods used for each Excel engine, refer to the pandas\n",
      ":ref:`user guide <io.excel_reader>`\n",
      "\n",
      "Examples\n",
      "--------\n",
      "The file can be read using the file name as string or an open file object:\n",
      "\n",
      ">>> pd.read_excel('tmp.xlsx', index_col=0)  # doctest: +SKIP\n",
      "       Name  Value\n",
      "0   string1      1\n",
      "1   string2      2\n",
      "2  #Comment      3\n",
      "\n",
      ">>> pd.read_excel(open('tmp.xlsx', 'rb'),\n",
      "...               sheet_name='Sheet3')  # doctest: +SKIP\n",
      "   Unnamed: 0      Name  Value\n",
      "0           0   string1      1\n",
      "1           1   string2      2\n",
      "2           2  #Comment      3\n",
      "\n",
      "Index and header can be specified via the `index_col` and `header` arguments\n",
      "\n",
      ">>> pd.read_excel('tmp.xlsx', index_col=None, header=None)  # doctest: +SKIP\n",
      "     0         1      2\n",
      "0  NaN      Name  Value\n",
      "1  0.0   string1      1\n",
      "2  1.0   string2      2\n",
      "3  2.0  #Comment      3\n",
      "\n",
      "Column types are inferred but can be explicitly specified\n",
      "\n",
      ">>> pd.read_excel('tmp.xlsx', index_col=0,\n",
      "...               dtype={'Name': str, 'Value': float})  # doctest: +SKIP\n",
      "       Name  Value\n",
      "0   string1    1.0\n",
      "1   string2    2.0\n",
      "2  #Comment    3.0\n",
      "\n",
      "True, False, and NA values, and thousands separators have defaults,\n",
      "but can be explicitly specified, too. Supply the values you would like\n",
      "as strings or lists of strings!\n",
      "\n",
      ">>> pd.read_excel('tmp.xlsx', index_col=0,\n",
      "...               na_values=['string1', 'string2'])  # doctest: +SKIP\n",
      "       Name  Value\n",
      "0       NaN      1\n",
      "1       NaN      2\n",
      "2  #Comment      3\n",
      "\n",
      "Comment lines in the excel input file can be skipped using the `comment` kwarg\n",
      "\n",
      ">>> pd.read_excel('tmp.xlsx', index_col=0, comment='#')  # doctest: +SKIP\n",
      "      Name  Value\n",
      "0  string1    1.0\n",
      "1  string2    2.0\n",
      "2     None    NaN\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\pault\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "pd.read_excel?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feat = df_well.drop('PP', axis=1) #replace 'PP' with the output label and change df to df_feat\n",
    "\n",
    "numerical_feature = [feature for feature in df.columns if df[feature].dtypes != 'O']\n",
    "discrete_feature=[feature for feature in numerical_feature if len(df[feature].unique())<25]\n",
    "continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]\n",
    "categorical_feature = [feature for feature in df.columns if feature not in numerical_feature]\n",
    "print(\"Numerical Features Count {}\".format(len(numerical_feature)))\n",
    "print(\"Discrete feature Count {}\".format(len(discrete_feature)))\n",
    "print(\"Continuous feature Count {}\".format(len(continuous_feature)))\n",
    "print(\"Categorical feature Count {}\".format(len(categorical_feature)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = ydata_profiling.ProfileReport(df) #replacing pandas_profiling with ydata_profiling\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "do_data_profiling(df, 'data_profiling.html') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML \n",
    "\n",
    "display(HTML(filename = 'data_profiling.html'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto EDA using dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale.show(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> EDA in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4a\"></a> <br>\n",
    "### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Basic formating (renaming cols, duplicates detection, datetime etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates \n",
    "df.drop_duplicates(inplace=True) \n",
    "\n",
    "#formating columns \n",
    "df.\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'price': 'selling_price', 'bedrooms': 'num_bedrooms'}, inplace=True)\n",
    "\n",
    "#replace non numeric columns\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> format datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "convert_timestamp(ts) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Remove unwanted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "df.drop(['id', 'date'], axis=1, inplace=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True: \n",
    "            print(f\"there is {df[col].isnull().sum()} NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)  \n",
    "\n",
    "missing_value_analysis (df)\n",
    "view_NaN(df) \n",
    "\n",
    "\n",
    "# np.argwhere(df.isnull().values)   #to find the index with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values \n",
    "# df.fillna(df.mean(), inplace=True) \n",
    "\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sklearn to handle missing values - (SimpleImputer, KNN-Imputer, Iterative Imputer, )\n",
    "\n",
    "\n",
    "#IterativeImputer: This function estimates missing values using a predictive model.\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "def impute_missing_values_iteratively(X): #or (X, Columns)\n",
    "    imputer = IterativeImputer(estimator = RandomForestRegressor())\n",
    "        \n",
    "    # select only the columns with missing values to be imputed\n",
    "    # X_cols = X[columns]\n",
    "    X_imputed = imputer.fit_transform(X) #or X_cols\n",
    "    return X_imputed\n",
    "\n",
    "impute_missing_values_iteratively(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize outliers\n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "visualize_outlier (df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Regression outliers using Cook's distance\n",
    "from yellowbrick.regressor import CooksDistance\n",
    "\n",
    "\n",
    "# Instantiate and fit the visualizer\n",
    "visualizer = CooksDistance()\n",
    "visualizer.fit(X, y)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Data Distribution of relevant columns before determining whether or not they should be classified as outliers\n",
    "\n",
    "\n",
    "# Plotting the data distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['max_power_bhp'], bins=20, color='blue')\n",
    "plt.xlabel('Max Power (bhp)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Data Distribution - Max Power (bhp)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detect_arbitrary(data,col,upper_fence,lower_fence):\n",
    "    '''\n",
    "    identify outliers based on arbitrary boundaries passed to the function.\n",
    "    '''\n",
    "\n",
    "    para = (upper_fence, lower_fence)\n",
    "    tmp = pd.concat([data[col]>upper_fence,data[col]<lower_fence],axis=1)\n",
    "    outlier_index = tmp.any(axis=1)\n",
    "    print('Num of outlier detected:',outlier_index.value_counts()[1])\n",
    "    print('Proportion of outlier detected',outlier_index.value_counts()[1]/len(outlier_index))    \n",
    "    return outlier_index, para "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to handle outliers, use any of Tukey's test, Kernel density estimation, Z-score method, Mahalanobis distance method,\n",
    "#Isolation Forest model, EllipticEnvelope.\n",
    "\n",
    "\n",
    "# Tukey's test: This statistical method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the median and works well for univariate datasets with normal distributions.\n",
    "\n",
    "# Kernel density estimation: This non-parametric method estimates the probability density function of a dataset \n",
    "#     and identifies outliers as values with low probability density, making it suitable for non-normal datasets.\n",
    "\n",
    "# Z-score method: This simple method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the mean and is widely used for datasets with normal distributions.\n",
    "\n",
    "# Mahalanobis distance method: This multivariate method identifies outliers based on the distance of each point from the \n",
    "#     centroid of the dataset and is effective for datasets with multivariate normal distributions.\n",
    "\n",
    "# Isolation Forest model: This machine learning algorithm identifies outliers by isolating them into a separate tree \n",
    "#     structure, making it suitable for high-dimensional feature spaces with both linear and non-linear relationships \n",
    "#     between features.\n",
    "\n",
    "# EllipticEnvelope: This multivariate method identifies outliers by fitting an ellipse to the data and identifying \n",
    "#     points that are outside the ellipse, making it effective for datasets with multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4b\"></a> <br>\n",
    "### Data Transformation (scaling, encoding categorical data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Extracting features from Dates, Mixed Variables etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df.resample('H').mean() \n",
    "df_day = df.resample('D').mean() \n",
    "df_month = df.resample('M').mean() \n",
    "df_year = df.resample('Y').mean()\n",
    "\n",
    "\n",
    "df['hour'] = df.index.hour \n",
    "df['day'] = df.index.day \n",
    "df['weekday'] = df.index.day_name() \n",
    "df['month'] = df.index.month \n",
    "df['year'] = df.index.year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Categorical Variable Encoding (data transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding: replaces each category with a numerical label. This technique is suitable for data \n",
    "#     where the categories have an intrinsic order, such as \"low,\" \"medium,\" and \"high.\" Works well with linear models\n",
    "\n",
    "# Ordinal encoding: assigns a numerical value to each category based on their frequency. This technique is suitable \n",
    "#     for data where the categories do not have an intrinsic order, but where their frequency may be informative.\n",
    "#     Suitable for non-linear models\n",
    "\n",
    "# One-hot encoding: creates a binary variable for each category, indicating its presence or absence. \n",
    "#     This technique is suitable for data where the categories do not have an intrinsic order and the \n",
    "#     number of categories is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : array-like of shape (n_samples, n_features)\n",
    "        The input dataset.\n",
    "    test_size : float, optional\n",
    "        The proportion of the dataset to include in the test set.\n",
    "    val_size : float, optional\n",
    "        The proportion of the dataset to include in the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : array-like of shape (n_train_samples, n_features)\n",
    "        The training input samples.\n",
    "    X_val : array-like of shape (n_val_samples, n_features)\n",
    "        The validation input samples.\n",
    "    X_test : array-like of shape (n_test_samples, n_features)\n",
    "        The test input samples.\n",
    "    y_train : array-like of shape (n_train_samples,)\n",
    "        The target values (class labels) for the training input samples.\n",
    "    y_val : array-like of shape (n_val_samples,)\n",
    "        The target values (class labels) for the validation input samples.\n",
    "    y_test : array-like of shape (n_test_samples,)\n",
    "        The target values (class labels) for the test input samples.\n",
    "    \"\"\"\n",
    "    #reset index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=test_size, random_state=42)\n",
    "\n",
    "    # Split the train set into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "    #show the shapes\n",
    "    print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler \n",
    "# # Set up the scaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Fit the scaler to the training set\n",
    "# scaler.fit(X_train) \n",
    "\n",
    "# # Transform the training and testing sets\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_val = scaler.transform(X_val) \n",
    "# X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Scaling (data transformation) - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if scaling only select features\n",
    "\n",
    "# columns_to_transform = ['year', 'km_driven', 'engine_CC', 'torque_Nm', \n",
    "#                         'seats', 'rpm', 'max_power_bhp', 'mileage_kmpl', 'vehicle_brand_target_encoded', \n",
    "#                         'vehicle_model_target_encoded']\n",
    "\n",
    "# # Create a StandardScaler object\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler on the selected columns\n",
    "# scaler.fit(X_train[columns_to_transform])\n",
    "\n",
    "# def scale_and_concat(df, scaler, columns_to_transform):\n",
    "#     \"\"\"\n",
    "#     Scales selected columns in a DataFrame using StandardScaler and concatenates them with the rest of the DataFrame.\n",
    "\n",
    "#     Args:\n",
    "#         df (pandas.DataFrame): The DataFrame containing the data.\n",
    "#         columns_to_transform (list): List of column names to be scaled.\n",
    "\n",
    "#     Returns:\n",
    "#         pandas.DataFrame: The concatenated DataFrame with scaled columns.\n",
    "#     \"\"\"\n",
    "#     # Create a copy of the original DataFrame\n",
    "#     df_concatenated = df.copy()\n",
    "\n",
    "#     # Scale the selected columns\n",
    "#     scaled_columns = scaler.transform(df[columns_to_transform])\n",
    "\n",
    "#     # Create a DataFrame with the scaled columns\n",
    "#     df_scaled = pd.DataFrame(scaled_columns, columns=columns_to_transform, index=df.index)\n",
    "\n",
    "#     # Concatenate the scaled columns with the rest of the DataFrame\n",
    "#     df_concatenated = pd.concat([df_concatenated.drop(columns=columns_to_transform, axis = 1), df_scaled], axis=1)\n",
    "\n",
    "#     return df_concatenated \n",
    "\n",
    "\n",
    "# X_train = scale_and_concat(X_train, scaler, columns_to_transform)\n",
    "# X_val = scale_and_concat(X_val, scaler, columns_to_transform) \n",
    "# X_test = scale_and_concat(X_test, scaler, columns_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the decision to scale or transform a variable should be based on \n",
    "    # its distribution, \n",
    "    # relationship with the target variable, and\n",
    "    # domain knowledge. \n",
    "# Scaling is not necessary if the variable follows a normal or approximately normal distribution; or if the variable has a linear \n",
    "# relationship with the target variable\n",
    "\n",
    "# Normalization - MinMax() - rescaling the values into a range of [0,1]. This process can be particularly useful when your features have \n",
    "# different scales and you want to standardize them without affecting the shape of the distributio\n",
    "\n",
    "# standardization - StandardScaler() - shifting the distribution of each attribute to have a mean of zero and a standard deviation of one \n",
    "# (unit variance). It is useful in cases where the data follows a Gaussian distribution\n",
    "\n",
    "\n",
    "#if scaling all the features \n",
    "def standardize_data(X_train, X_val, X_test): \n",
    "    \"\"\"\n",
    "    Standardizes the training and testing data using the mean and standard deviation\n",
    "    learned from the training set.\n",
    "    \n",
    "    Args:\n",
    "    - X_train: numpy array or pandas dataframe, training data\n",
    "    - X_test: numpy array or pandas dataframe, testing data\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled: numpy array or pandas dataframe, standardized training data\n",
    "    - X_test_scaled: numpy array or pandas dataframe, standardized testing data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    # Set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the training set\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    # Transform the training and testing sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return scaler, X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "from typing import List \n",
    "def to_DataFrame(Dataset, columns:List) -> DataFrame: \n",
    "    df = pd.DataFrame(Dataset, columns=columns)\n",
    "    return df \n",
    "\n",
    "\n",
    "scaler, X_train, X_val, X_test = standardize_data(X_test, X_test) \n",
    "X_train = to_DataFrame(X_train, columns= continuous_feature)\n",
    "X_val = to_DataFrame(X_val, columns= continuous_feature)\n",
    "X_test = to_DataFrame(X_test, columns= continuous_feature) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Variable Transformation - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the decision to scale or transform a variable should be based on its distribution, relationship with the \n",
    "# target variable, and domain knowledge. Scaling is not necessary if the variable follows a normal or approximately \n",
    "# normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable transformation involves transforming the values of variables to make them more suitable for analysis\n",
    "#the idea is to make the variables normally/gaussian distributed. Hence, \n",
    "\n",
    "#first step is to assess normality using a histogram or QQ-plot (to explore the variable distribution)\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.title(f\"Histogram of {variable}\")\n",
    "\n",
    "    # q-q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot of {variable}\")\n",
    "\n",
    "    # check for skewness\n",
    "    skewness = df[variable].skew()\n",
    "    if skewness > 0:\n",
    "        skew_type = \"positively skewed\"\n",
    "    elif skewness < 0:\n",
    "        skew_type = \"negatively skewed\"\n",
    "    else:\n",
    "        skew_type = \"approximately symmetric\"\n",
    "        \n",
    "    # print message indicating skewness type\n",
    "    print(f\"The variable {variable} is {skew_type} (skewness = {skewness:.2f})\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# # Check function output\n",
    "# diagnostic_plots(X, \"MedInc\")\n",
    "\n",
    "\n",
    "#use this to make diagnostics plot for all variables\n",
    "for feature in continuous_feature:\n",
    "    print(feature)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[feature], dist=\"norm\", plot=plt) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_plots(df, variable):     \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.show()\n",
    "\n",
    "for feature in continuous_feature:\n",
    "    print(feature)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[feature], dist=\"norm\", plot=plt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the variables are NOT normally distributed, we then transform it. It is necessary to test several variable \n",
    "# transformation methods, and choose the best for that feature. One variable transformation method is log_transform\n",
    "\n",
    "#NB: if data is positively skewed (right skewed), use (logarithmic, reciprocal, or square root transformation)\n",
    "    #if data is negatively skewed (left skewed), use (Box-Cox or Yeo-Johnson transformations)\n",
    "#log transform \n",
    "def log_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the natural logarithm function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_log = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_log\n",
    "\n",
    "#perform transformation on the variables that are not normally distributed\n",
    "X_train = log_transform(X_train, columns = ['Vp', 'Caliper'])\n",
    "X_val = log_transform(X_val, columns = ['Vp', 'Caliper'])\n",
    "X_test = log_transform(X_test, columns = ['Vp', 'Caliper']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df_log, columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Discretization (data transformation - apply to train, and then to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization in machine learning is the process of transforming continuous variables into discrete or \n",
    "# categorical variables. This process involves dividing the range of a continuous variable into a finite number of \n",
    "# intervals or bins, and then assigning each observation to a particular bin based on the value of the continuous \n",
    "# variable. \n",
    "\n",
    "#Discretization approaches: equal width, equal frequency, K means, Decision Trees\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4c\"></a> <br>\n",
    "### Handling Imbalanced Data and Biases (Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to train, and then to test \n",
    "\n",
    "#Imbalanced data refers to a situation where the number of observations in one class or category is much larger \n",
    "# or smaller than the number of observations in other classes or categories. Imbalanced data can pose challenges \n",
    "# in machine learning because it can lead to biased models that perform poorly on the minority class.\n",
    "\n",
    "#we typically look at the 'target data' when checking for imbalanced data. However, it is also important \n",
    "# to consider the features\n",
    "\n",
    "def check_imbalance(dataset, columns=None, threshold=10):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and one or more columns as input and returns True if any of the specified columns\n",
    "    are imbalanced, False otherwise. A column is considered imbalanced if the percentage of the minority class is less\n",
    "    than the specified threshold.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, use all columns except for the last one as the features\n",
    "    if columns is None:\n",
    "        features = dataset.iloc[:, :-1]\n",
    "        columns = features.columns\n",
    "    \n",
    "    # Check the imbalance of each specified column\n",
    "    for col in columns:\n",
    "        # Get the counts of each class in the column\n",
    "        class_counts = dataset[col].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each class in the column\n",
    "        class_percentages = class_counts / len(dataset) * 100\n",
    "\n",
    "        # Plot the class percentages\n",
    "        plt.bar(class_counts.index, class_percentages)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the column is imbalanced\n",
    "        minority_class = class_counts.index[-1]\n",
    "        minority_class_percentage = class_percentages.iloc[-1]\n",
    "        if minority_class_percentage < threshold:\n",
    "            print(f'{col} is imbalanced. Minority class: {minority_class}, Percentage: {minority_class_percentage:.2f}%')\n",
    "            return True\n",
    "\n",
    "    # If none of the specified columns are imbalanced, return False\n",
    "    print('No imbalance found.')\n",
    "    return False\n",
    "\n",
    "check_imbalance(df, columns=['target'], threshold=10) \n",
    "\n",
    "\n",
    "#OR\n",
    "\n",
    "from yellowbrick.target import ClassBalance \n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance(labels=[\"draw\", \"loss\", \"win\"])\n",
    "visualizer.fit(y_train, y_test)        # Fit the data to the visualizer (you can also use visualizer.fit(y))\n",
    "visualizer.show()                       # Finalize and render the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is imbalance, you can handle it by over-sampling or under-sampling the dataset\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def handle_imbalanced_data(X, y, strategy='over-sampling'):\n",
    "    \"\"\"\n",
    "    Handle imbalanced data using imblearn library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array-like of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y: array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    strategy: str, default='over-sampling'\n",
    "        The strategy to use for handling imbalanced data. Possible values are\n",
    "        'over-sampling' and 'under-sampling'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled: array-like of shape (n_samples_new, n_features)\n",
    "        The resampled input data.\n",
    "    y_resampled: array-like of shape (n_samples_new,)\n",
    "        The resampled target values.\n",
    "    \"\"\"\n",
    "    if strategy == 'over-sampling':\n",
    "        # Initialize the RandomOverSampler object\n",
    "        ros = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    elif strategy == 'under-sampling':\n",
    "        # Initialize the RandomUnderSampler object\n",
    "        rus = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Possible values are 'over-sampling' and 'under-sampling'.\")\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4d\"></a> <br>\n",
    "### Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step involves reducing the dimensionality of the data by selecting a subset of the original features or \n",
    "# by transforming the data using techniques like Principal Component Analysis (PCA).\n",
    "\n",
    "#Feature selection\n",
    "    # Removing features with low variance (VarianceThreshold)\n",
    "    # Univariate Feature Selection (SelectKBest, SelectPercentile, GenericUnivariateSelect)\n",
    "    # Recursive Feature Elimination (RFE, RFECV)        RFECV - RFE cross validation\n",
    "    # Feature selection using SelectFromModel (SelectFromModel) - use L1-based (Lasso, Ridge, ElasticNet) or Tree-based\n",
    "    # Sequential Feature Selection (SequentialFeatureSelector) - SFS can be either forward or backward\n",
    "\n",
    "\n",
    "#Feature extraction\n",
    "    # Principal Component Analysis (PCA)\n",
    "    # Independent Component Analysis (ICA)\n",
    "    # t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## Selecting and Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you choose a ML technique to train the model, consider if your ML technique is resistant to the following:\n",
    "#     Missing data\n",
    "#     Data imbalance\n",
    "#     Feature Scaling\n",
    "#     Categorical Data\n",
    "#     Outliers\n",
    "#     Dimensionality (refers to the number of features or variables in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use AutoML (check for PyCaret and Auto SKlearn - see AutoML file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> AutoML using Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(df, columns):\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns = [columns], axis = 1)\n",
    "    return df\n",
    "\n",
    "y_train_pycaret = reset_index(y_train)\n",
    "y_val_pycaret = reset_index(y_val) \n",
    "y_test_pycaret = reset_index(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pycaret\n",
    "from pycaret.classification import * \n",
    "from pycaret.classification import setup, compare_models, create_model, tune_model, plot_model, evaluate_model, save_model\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True) \n",
    "X['target'] = y \n",
    "# Initialize classification setup \n",
    "# clf1 = setup(data=X, target='target', train_size = 0.8, \n",
    "#              preprocess = True, polynomial_features = True, \n",
    "#              polynomial_degree = 2, fix_imbalance = True,\n",
    "#              fix_imbalance_method = 'SMOTE', feature_selection = True,\n",
    "#              feature_selection_method = ' ', feature_selection_estimator = ,\n",
    "#              n_features_to_select = 0.2) \n",
    "\n",
    "\n",
    "clf1 = setup(data=X, target='target', train_size = 0.8, session_id = 123)\n",
    "# all_models = models()   #use this to visualize a table of models available in the model library.\n",
    "\n",
    "# Compare models \n",
    "compare_results = compare_models(n_select=5)    #the best 5 models will be highlighted\n",
    "\n",
    "compare_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for model in compare_results:\n",
    "    evaluate_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = create_model('knn')     #change knn to any of the top 5 models from above\n",
    "            # from pycaret.regression import models     #change to classification when needed, then do: exp = setup(X, y)\n",
    "            # regression_models = models() # Get all regression model estimators  \n",
    "            # print(regression_models) # Display the list of model names\n",
    "\n",
    "\n",
    "# # Tune the model\n",
    "tuned_model = tune_model(model)\n",
    "\n",
    "# # Evaluate the model\n",
    "evaluate_model(tuned_model)\n",
    "\n",
    "# # Fit the model\n",
    "final_model = tune_model(tuned_model)\n",
    "\n",
    "# Save the final model in the \"ML\" folder\n",
    "model_path = 'models/pycaret_ExtraTreesRegressor_r2'\n",
    "save_model(final_model, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Then select the preferred model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest classifier on the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# # Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[1,1], random_state=1)\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = roc_auc_score(train_y, train_preds)\n",
    "        test_loss = roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "y_pred, test_loss, test_preds2, clf = runRF(\n",
    "                                            X_train,\n",
    "                                            y_train,\n",
    "                                            X_val,\n",
    "                                            y_val=None,\n",
    "                                            test_X2=None,\n",
    "                                            rounds=100,\n",
    "                                            depth=20,\n",
    "                                            leaf=10,\n",
    "                                            feat=0.2,\n",
    "                                            min_data_split_val=2,\n",
    "                                            seed_val=0,\n",
    "                                            job=-1,\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification performance two-class report\n",
    "\n",
    "\n",
    "y_pred_proba,ACC,PC,RC,FS,AP,roc_auc,gini = print_classification_performance2class_report(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#classification performance multi-class report\n",
    "\n",
    "\n",
    "y_pred, ACC, PC, RC, FS, roc_auc = print_classification_performance_multiclass_report(model, X_test, y_test)\n",
    "\n",
    "# ROC AUC is calculated using a One-vs-Rest approach. This involves treating each class as a binary classification against all other classes.\n",
    "# Precision, recall, and F1 score are calculated as a weighted average to account for class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression performance report \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "mse, rmse, r2, y_pred = print_regression_performance_report(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "    # Error = Actual value - Predicted value\n",
    "\n",
    "    # MSE (Mean Square Error)\n",
    "        # The square of the error over all samples is called Mean Squarred Error(MSE).\n",
    "        # MSE = SQUARE(Actual value - Predicted value)/Number of Samples\n",
    "    #RMSE (Root Mean Square Error)\n",
    "    # MAE (Mean Absolute Error)\n",
    "        # MAE = ABSOLUTE (Actual value - Predicted Value)\n",
    "\n",
    "\n",
    "#Classification\n",
    "    #Accuracy: (TP + TN / {TP + FP + TN + FN})\n",
    "        # Measures the proportion of correct predictions among the total number of cases processed.\n",
    "        # Useful for balanced datasets but can be misleading in the presence of class imbalances.\n",
    "        \n",
    "    #Precision: (TP / {TP + FP})\n",
    "        # Indicates the proportion of positive identifications that were actually correct.\n",
    "        # Particularly important in scenarios where false positives are a significant concern.\n",
    "        \n",
    "    #Recall (Sensitivity): (TP / {TP + FN})\n",
    "        # Measures the proportion of actual positives that were correctly identified.\n",
    "        # Critical in situations where missing a positive is significantly worse than falsely identifying a negative.\n",
    "    \n",
    "    # Specitivity: (TN/ {TN+FP})\n",
    "        # Specificity measures the proportion of actual negatives that are correctly identified as such.\n",
    "        # High specificity implies the model is effective in ruling out the condition when itâ€™s not present\n",
    "        \n",
    "    #F1 score: (2 * {[PC*RC] / [PC+RC]} )\n",
    "        # A harmonic mean of precision and recall. Provides a balance between precision and recall in one number.\n",
    "        # Particularly useful when you need to compare two or more models or when thereâ€™s an uneven class distribution.\n",
    "        \n",
    "    #AUC-ROC curve\n",
    "        # It plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at various threshold settings. \n",
    "        # The Area Under the Curve (AUC) quantifies the overall ability of the test to discriminate between positive and negative classes\n",
    "        # Higher the AUC, the better the model is at distinguishing between positive and negative classes\n",
    "\n",
    "    # Precision-Recall Curve:\n",
    "        # Shows the tradeoff between precision and recall for different threshold values.\n",
    "        # A high area under the curve represents both high recall and high precision.\n",
    "        # More informative than ROC curves in case of highly imbalanced datasets.\n",
    "        \n",
    "    # Confusion Matrix:    \n",
    "        # True Positives(TP): Number of samples that are correctly classified as positive, and their actual label is positive.\n",
    "        # False Positives (FP): Number of samples that are incorrectly classified as positive, when in fact their actual label \n",
    "            # is negative.\n",
    "        # True Negatives (TN): Number of samples that are correctly classified as negative, and their actual label is negative.\n",
    "        # False Negatives (FN): Number of samples that are incorrectly classified as negative, when in fact their actual label \n",
    "            # is positive.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Class Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "# classes = [\"apple\", \"kiwi\", \"pear\", \"banana\", \"orange\"]\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassPredictionError(model, classes=classes)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Learning Curve (Access the Bias and Variance) - Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Bias` - error in the training data\n",
    "\n",
    "- `Variance` -> difference in the errors between the train and test data. (i.e.,  it examines how the model's performance varies between the training data and unseen data (like a validation set).  If the model performs well on the training data but poorly on the validation/test data, it suggests high variance)\n",
    "\n",
    "\n",
    "`High Bias (will also have High Variance) -> Underfitting:` The model is too simple and doesn't capture the complexities of the data well, leading to poor performance on both training and testing datasets.\n",
    "\n",
    "`Low Bias + High Variance -> Overfitting:` The model is too complex, fitting too closely to the training data, including its noise and outliers. It performs well on training data but poorly on unseen data.\n",
    "\n",
    "`Low Bias + Low Variance -> Ideal Model:` This is the desired outcome. The model accurately captures the underlying patterns in the data (low bias) and generalizes well to unseen data (low variance).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"learning-curves.png\" alt=\"Example Image\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check Bias and Variance using Learnng Curve \n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10) # Define the training set sizes to plot the learning curve\n",
    "\n",
    "def cv_learning_curve(model, X, y, cv, train_sizes):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "                                                #scoring parameter -  #https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter \n",
    "    train_mean = np.mean(-train_scores, axis=1)\n",
    "    train_std = np.std(-train_scores, axis=1)\n",
    "    test_mean = np.mean(-test_scores, axis=1)\n",
    "    test_std = np.std(-test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Error')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Validation Error')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_sizes, train_mean, train_std, test_mean, test_std\n",
    "\n",
    "\n",
    "\n",
    "# Generate learning curve plot\n",
    "train_sizes, train_mean, train_std, test_mean, test_std = cv_learning_curve(model, X, y, cv, train_sizes)\n",
    "\n",
    "\n",
    "# To fix high bias (underfitting):\n",
    "    # get additional features or increasing the size of the model \n",
    "    # Adding polynomial features is a form of feature engineering that can increase the complexity of the model\n",
    "    # decrease the regularization parameter (lambda) to allow the model's learning algorithm to fit the data more flexibly, \n",
    "        # thereby potentially reducing bias\n",
    "\n",
    "# To fix high variance (overfitting):\n",
    "    # Obtaining more training samples can help the model generalize better.\n",
    "    # Simplifying the model by reducing the number of features (feature selection) can prevent the model from fitting noise in the \n",
    "        # training data.\n",
    "    # Increasing the regularization parameter (lambda) adds a penalty to the model complexity, which can help in preventing overfitting.\n",
    "\n",
    "\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "visualizer = LearningCurve(\n",
    "    model, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
    "        )\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, cross_val_score, KFold, train_test_split\n",
    "from yellowbrick.model_selection import CVScores #visualizing the cross validation scores\n",
    "\n",
    "#check Bias and Variance using Cross Validation\n",
    "\n",
    "cv = 5  #or # Create a cross-validation object: \n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "def cv_bias_variance(model, X, y, cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')   \n",
    "    train_error = -scores.mean()\n",
    "    val_error = -scores.std()\n",
    "    return train_error, val_error, scores\n",
    "\n",
    "# options to replace scoring:\n",
    "#     regression: r2, neg_mean_absolute_error, explained_variance, neg_root_mean_squared_error, etc.\n",
    "#     classification: accuracy, f1, roc_auc, precision, recall, etc.  \n",
    "\n",
    "\n",
    "# Calculate the mean training and validation error scores\n",
    "train_error, val_error, scores = cv_bias_variance(model, X, y, cv)\n",
    "print(\"Mean training error:\", train_error)\n",
    "print(\"Mean validation error:\", val_error)\n",
    "\n",
    "\n",
    "visualizer = CVScores(model, cv=cv, scoring='r2')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot Validation Curves (to analyse the impact of each Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to analyze the impact of varying hyperparameter values on the model's performance using Validation curve\n",
    "#the validation curve is useful for hyperparameter tuning, while the learning curve is used to assess bias and variance.\n",
    "\n",
    "# Using validation curves is a fundamental aspect of machine learning model tuning, as it provides valuable insights into how \n",
    "# hyperparameters influence model performance, aiding in the selection of the most appropriate model settings.\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "\n",
    "#the hyperparameter used here is 'max_depth'. A hyperparameter for the model ExtraTreeRegressor\n",
    "\n",
    "plot_validation_curve(model, X_train, y_train, param_name=\"max_depth\", cv=5, scoring=\"r2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import ValidationCurve \n",
    "\n",
    "# cv = StratifiedKFold(4)\n",
    "viz = ValidationCurve(\n",
    "    DecisionTreeRegressor(), param_name=\"max_depth\",\n",
    "    param_range=np.arange(1, 11), cv=10, scoring=\"r2\"\n",
    ")\n",
    "\n",
    "# Fit and show the visualizer\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Analyze Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the errors are normally distributed around zero, it may indicate that the model is making unbiased predictions. \n",
    "# If there is a pattern or trend in the errors, it may suggest that the model has systematic biases or is making \n",
    "# consistent errors in certain regions of the input space\n",
    "\n",
    "\n",
    "analyze_error_distribution(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Error Analysis - Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error analysis is the process of analyzing the errors made by a machine learning model and identifying the patterns \n",
    "# or trends that may be causing the errors. The goal of error analysis is to gain insight into the behavior of the \n",
    "# model and identify areas for improvement. \n",
    "\n",
    "# The steps involved in error analysis:\n",
    "    # 1. Collect error data - Gather instances where the model made incorrect predictions.\n",
    "    # 2. Categorize errors - Classify errors into meaningful categories.\n",
    "    # 3. Identify patterns - Look for commonalities or trends among the errors.\n",
    "    # 4. Analyze causes - Investigate potential reasons behind these patterns.\n",
    "    # 5. Prioritize fixes - Decide which errors to address first based on their impact.\n",
    "\n",
    "    \n",
    "# Based on the insights gained from the error analysis, you can perform the following.\n",
    "\n",
    "# False negatives: \n",
    "    # To fix this issue, you may consider the following:\n",
    "        # Increase the weight of the features that are more indicative of churn for low-usage customers, \n",
    "            # such as frequency of usage or specific product usage. (adjust the model parameters)\n",
    "        # Add new features that may be predictive of churn, such as customer sentiment or customer service interactions.\n",
    "        # Use a different model architecture that is better suited for handling imbalanced data, such as a decision tree \n",
    "            # or ensemble model. \n",
    "\n",
    "\n",
    "# False positives:\n",
    "    # To fix this issue, you may consider the following:\n",
    "        # Decrease the weight of features that are causing false positives, such as age or income, if they are not as \n",
    "            # indicative of churn for low-usage customers. (adjust the model parameters)\n",
    "        # Remove features that are causing false positives altogether, if they are not providing significant value to the \n",
    "            # model.\n",
    "        # Increase the size of the training dataset to capture a more representative sample of customers who do not churn, \n",
    "            # which may help the model learn more accurately which customers are likely to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix to visualize false positives and false negatives\n",
    "    #By default, scikit-learn will assume that the \"positive\" class is the last label (or highest label value) \n",
    "    # in the list of labels. [0, 1] where 1 is Positive and is the class_of_interest.\n",
    "\n",
    "\n",
    "class_names = [0, 1] #or iris().target_names #this is an example and should be edited. [0, 1] for binary classification\n",
    "class_of_interest = 1 #this selects a specific class of interest other than 1 or the highest value. \n",
    "                        #always select the highest one because that is what Scikit learn uses. \n",
    "\n",
    "def false_positives(X_test, y_true, y_pred, classes):\n",
    "    \"\"\" \n",
    "    This function identifies and plots the false positives in a classification problem. \n",
    "    \"\"\" \n",
    "    fp_indices = np.where((y_true != class_of_interest) & (y_pred == class_of_interest))[0] \n",
    "    fp_features = X_test[fp_indices] # assuming X_test is a numpy array of input data \n",
    "    # fp_features = X_test.iloc[fp_indices]\n",
    "    fp_labels = y_pred[fp_indices] # assuming y_pred is a numpy array of predicted labels \n",
    "    # fp_labels = pd.Series(y_pred).iloc[fp_indices]\n",
    "\n",
    "    print(\"False positives: \", len(fp_indices))\n",
    "    return fp_features, fp_labels\n",
    "\n",
    "\n",
    "#false negatives \n",
    "def false_negatives(X_test, y_true, y_pred, classes):\n",
    "    \"\"\" \n",
    "    This function identifies and plots the false negatives in a classification problem. \n",
    "    \"\"\" \n",
    "    fn_indices = np.where((y_true == class_of_interest) & (y_pred != class_of_interest))[0] \n",
    "    fn_features = X_test[fn_indices] # assuming X_test is a numpy array of input data\n",
    "    # fn_features = X_test.iloc[fn_indices] \n",
    "    fn_labels = y_pred[fn_indices] # assuming y_pred is a numpy array of predicted labels \n",
    "    # fn_labels = pd.Series(y_pred).iloc[fn_indices]\n",
    "\n",
    "    print(\"False negatives: \", len(fn_indices))\n",
    "    return fn_features, fn_labels\n",
    "\n",
    "\n",
    "# Plot the confusion matrix to evaluate the performance of the model\n",
    "plot_confusion_matrix(y_test, y_pred, classes=classes,\n",
    "                      title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))\n",
    "\n",
    "# Identify and plot the false positives\n",
    "X_fp, y_fp = false_positives(X_test, y_test, y_pred, class_names)\n",
    "\n",
    "# Identify and plot the false negatives\n",
    "X_fn, y_fn = false_negatives(X_test, y_test, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the most important features in the model\n",
    "\n",
    "#depending on the results from the bias and variance tests, there may be need to assess which features\n",
    "# are the most important in the ML model\n",
    "\n",
    "\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "feature_importance(model,X_train)  \n",
    "\n",
    "\n",
    "\n",
    "#OR\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "viz = FeatureImportances(model, labels=labels, relative=False)\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualize Model Performance during Feature Selection/Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "visualizer = RFECV(RandomForestClassifier(), cv=cv, scoring='f1_weighted')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Selection and Extraction - (Fix High Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing High Variance in Models through Feature Selection/Extraction\n",
    "\n",
    "# Overview\n",
    "    # Goal: Improve model generalization by reducing complexity or enhancing informative representations.\n",
    "    # Application: Apply techniques to training data, then to test data.\n",
    "    \n",
    "# Initial Steps\n",
    "    # Assess feature importance.\n",
    "    # Options:\n",
    "        # Add or remove features based on their importance.\n",
    "        # Consider creating polynomial features for more complex relationships.\n",
    "\n",
    "# Feature Selection/Extraction Strategies\n",
    "\n",
    "    # Step 1: Manual Feature Adjustment\n",
    "        # Add or remove features based on domain knowledge or preliminary analysis.\n",
    "        \n",
    "    # Step 2: Automated Techniques (start with feature selection)\n",
    "    \n",
    "        # Feature Selection Methods:\n",
    "            # VarianceThreshold: Remove features with low variance.\n",
    "            # Univariate Selection: Use methods like SelectKBest, SelectPercentile, or GenericUnivariateSelect.\n",
    "            # Recursive Feature Elimination: RFE or RFECV (with cross-validation).\n",
    "            # Model-Based Selection: Use SelectFromModel with L1-based (Lasso, Ridge, ElasticNet) or tree-based methods.\n",
    "            # Sequential Feature Selector: Forward or backward selection (SFS).\n",
    "            \n",
    "        # Feature Extraction Methods:\n",
    "            # Principal Component Analysis (PCA).\n",
    "            # Independent Component Analysis (ICA).\n",
    "            # t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "        \n",
    "        # Common Choices: PCA for extraction and SelectKBest for selection.\n",
    "\n",
    "# Workflow for Model Improvement \n",
    "    # Build and test a model with normal data.\n",
    "    # If accuracy is not satisfactory, perform feature selection and retest.\n",
    "        # Optionally, add polynomial features to the selected features and retest.\n",
    "    # If needed, proceed to feature extraction and retest.\n",
    "        # Again, consider adding polynomial features and retesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adding Polynomial Features (Fix High Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Polynomial Features (to Fix High Bias) - do this only if there is high variance\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df_polynomial = add_polynomial_features_sklearn(df, degree, columns=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Performing Regularization (fix High Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to fix High Bias or Variance is to perform regularization on the model.\n",
    "#this would involve increasing or decreasing the regularization parameter (lambda) to fix high variance or bias\n",
    "\n",
    "\n",
    "# By tuning the hyperparameters of the model using cross-validation, \n",
    "# we would have effectively applied regularization to the model, which can help to reduce overfitting and improve \n",
    "# its generalization performance.\n",
    "\n",
    "#Hence the next step is MODEL OPTIMIZATION. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "## Model Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto Hyperparameter Optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import optuna\n",
    "\n",
    "SVC().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level for Optuna to WARNING\n",
    "# logging.getLogger('optuna').setLevel(logging.WARNING) \n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters' search space\n",
    "    C = trial.suggest_loguniform('C', 1e-4, 1e4)\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    degree = trial.suggest_int('degree', 1, 5) if kernel == 'poly' else 3  # degree is only used for 'poly' kernel\n",
    "\n",
    "    # Create the SVM model\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, probability=True)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10,random_state=11,shuffle=True) \n",
    "    \n",
    "    # Perform cross-validation and compute the average AUC score\n",
    "    scores = cross_val_score(model, train_data, train_labels, scoring='roc_auc', n_jobs=-1, cv=cv)\n",
    "    avg_auc_test = np.mean(scores)\n",
    "\n",
    "    return avg_auc_test\n",
    "\n",
    "# Create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_hyperparams = study.best_trial.params\n",
    "# print('Best Hyperparameters:', best_hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters obtained from Optuna\n",
    "best_C_svm = best_hyperparams['C']\n",
    "best_kernel = best_hyperparams['kernel']\n",
    "best_gamma = best_hyperparams['gamma']\n",
    "best_degree = best_hyperparams['degree'] \n",
    "\n",
    "\n",
    "print('Best C (svm): {}'.format(best_C_svm))\n",
    "print('Best kernel: {}'.format(best_kernel))\n",
    "print('Best gamma: {}'.format(best_gamma))\n",
    "print('Best degree: {}'.format(best_degree))\n",
    "\n",
    "tuned_svm_model = SVC(C=best_C_svm, kernel=best_kernel, gamma=best_gamma, degree = best_degree, probability=True)\n",
    "\n",
    "tuned_svm_model.fit(train_data, train_labels)\n",
    "\n",
    "dump(tuned_svm_model, 'models/tuned_model/tuned_support_vector.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred,ACC,PC,RC,FS,AP,roc_auc,gini = print_classification_performance2class_report(tuned_svm_model,valid_data,valid_labels) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "## Model Deployment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Save the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a trained scikit-learn model to disk using joblib.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        joblib.dump(model, filename)\n",
    "        # joblib.dump(pipeline, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {filename}: {e}\")\n",
    "\n",
    "save_model(model, 'model.joblib')\n",
    "\n",
    "\n",
    "# model = joblib.load() # to load the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Build a Data pre-processing pipeline for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "#Learn to use the following: Pipeline, FeatureUnion, FunctionTransformer, ColumnTransformer\n",
    "# set_config(display='diagram')   #to visualize pipeline    \n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('breast_cancer.joblib')\n",
    "\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "def preprocessing_data (user_inputs:float) -> np.ndarray:\n",
    "\n",
    "    # Create a DataFrame from the user inputs\n",
    "    X_new = pd.DataFrame(user_inputs, columns=X.columns,index=[0])\n",
    "\n",
    "    # Define transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=2)),\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "    # Define pipeline using FeatureUnion\n",
    "    preprocessor = FeatureUnion(transformer_list=[\n",
    "        ('numeric_transformer', numeric_transformer),\n",
    "        ('categorical_transformer', categorical_transformer),\n",
    "    ])\n",
    "\n",
    "    # Fit and transform preprocessor on test data\n",
    "    X_test = preprocessor.fit_transform(X_new)\n",
    "\n",
    "    return X_test \n",
    "\n",
    "# #use this to visualize the pipelines as a diagram \n",
    "# from sklearn import set_config\n",
    "# set_config(display='diagram')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Proof of Concept (POC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Interactive Interface using Gradio\n",
    "\n",
    "#before deploying your model to production, you may need to show a POC (proof of concept) i.e a prototype\n",
    "#use Gradio library for interfaces for your ML model \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the model here\n",
    "\n",
    "# https://dashboard.render.com/\n",
    "# sign-in with Github\n",
    "#or use Heroku, GCP, AWS, AZUre, IBM Watson etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[1,1], random_state=1)\n",
    "feature_names = ['Feature_{}'.format(i) for i in range(X.shape[1])] # Create feature names\n",
    "\n",
    "\n",
    "#you can use both RandomGridSearch and GridSearch to optimize a model. Use RandomGridSearch first, then GridSearch\n",
    "rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "rfc_random = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), random_grid, verbose=2, cv=3, refit=True)\n",
    "        # estimator: This is the machine learning model or estimator that you want to optimize using hyperparameter tuning\n",
    "        # param_distributions: This parameter specifies the hyperparameter space to be searched during the random search\n",
    "        # n_iter: This specifies the number of random combinations of hyperparameter values to try during the search.\n",
    "        # cv: This parameter determines the number of folds in the cross-validation process.\n",
    "        # verbose: This controls the verbosity of the output during the hyperparameter search. \n",
    "        #     A higher value, such as verbose=2, means more detailed output will be displayed during the search.\n",
    "        # random_state: This parameter sets the random seed for reproducibility\n",
    "        # n_jobs: This specifies the number of CPU cores to use for parallelization during the hyperparameter search. \n",
    "        #     A value of -1 (n_jobs=-1) means that all available CPU cores will be used.\n",
    "        # scoring: This parameter specifies the scoring metric used to evaluate the performance of the model \n",
    "        #     with different hyperparameter values. It can be set to a string representing a scoring metric, \n",
    "        #     such as 'accuracy', 'precision', 'recall', 'f1', etc., or an object of a custom scoring function\n",
    "        # refit: This parameter determines whether the best hyperparameters found during the search should be used to \n",
    "        #     refit the model on the entire dataset after the search is complete.\n",
    "        \n",
    "# Cross-validation techniques (three of the most common):\n",
    "#     K-fold Cross-Validation: It divides the data into k equally sized folds and performs training and testing on \n",
    "#         k iterations. It is widely used due to its simplicity and provides a good balance between bias and variance.\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     Stratified K-fold Cross-Validation: It is similar to K-fold cross-validation, but it ensures that each fold has an \n",
    "#         approximately equal distribution of target classes, making it suitable for imbalanced datasets.\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     Time Series Cross-Validation: It is used for time series data, where the order of data points matters. \n",
    "#         It involves using a sliding time window to create overlapping train and test sets, taking into account \n",
    "#         temporal dependencies.\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#      Leave-One-Out Cross-Validation (LOOCV): It is a special case of K-fold cross-validation where k is set to the \n",
    "#         total number of samples, resulting in each sample being used as a test set once. It is computationally expensive \n",
    "#         but can be useful for small datasets.\n",
    "        from sklearn.model_selection import LeaveOneOut\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
