{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open VSCode and create a new project folder for your machine learning project.\n",
    "# Open the terminal in VSCode by going to Terminal > New Terminal.\n",
    "# Create a new environment using conda or pip. For example, to create a new environment with conda:\n",
    "#     conda create --name myenv\n",
    "#     conda activate myenv\n",
    "# Install the necessary packages for your machine learning project. For example, to install scikit-learn:\n",
    "#     conda install pandas numpy scikit-learn flask\n",
    "#     pip install -r requirements.txt             use this if you had initially loaded some packages to the file\n",
    "# Export the dependencies of your project by running the command:\n",
    "#     pip freeze > requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Table of Content:\n",
    "* [Import Libraries](#1)\n",
    "* [Load Data](#2)\n",
    "* [Exploratory Data Analysis (EDA)](#3)\n",
    "* [Data Preprocessing](#4)\n",
    "    * [Data Cleaning](#4a)\n",
    "    * [Data Transformation](#4b)\n",
    "    * [Handling Imbalanced Data](#4c)\n",
    "    * [Data Reduction](#4d)\n",
    "* [Selecting and Training the Model](#5) \n",
    "* [Model Evaluation](#6) \n",
    "* [Feature Engineering](#7) \n",
    "* [Model Optimization](#8) \n",
    "* [Model Deployment](#9) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis      \n",
    "import pandas as pd          # data analysis library for handling structured data             \n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "import ydata_profiling  #for auto EDA\n",
    "import dtale        #for auto Exploratory Data Analysis\n",
    "from metrics import *\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "import missingno as msno    #visualize missing data\n",
    "\n",
    "# Machine Learning/Time Series \n",
    "from xgboost import XGBRegressor \n",
    "#ML - Preprocessing data \n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer #variable transformation \n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder # Preprocessing feature scaling/categorical encoding\n",
    "from sklearn.preprocessing import Normalizer, Binarizer \n",
    "from sklearn.model_selection import train_test_split    #split data into train and test \n",
    "#ML - Handling imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler  \n",
    "#ML - Create your model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  #linear and logistics regression models\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "#ML - Evaluate model performance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, accuracy_score, classification_report \n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score, homogeneity_score \n",
    "from sklearn.metrics import (roc_auc_score,roc_curve,precision_recall_curve, auc,\n",
    "                             classification_report, confusion_matrix, average_precision_score,\n",
    "                             accuracy_score,silhouette_score,mean_squared_error)\n",
    "from inspect import signature \n",
    "#ML - Tune your model\n",
    "import optuna\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 15) \n",
    "pd.set_option('display.max_columns', 500) \n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiment_1 = pd.read_csv('exp1', index_col='time', parse_dates=True)\n",
    "experiment_2 = pd.read_csv('exp2', index_col='time', parse_dates=True)\n",
    "# Repeat for all experiments...\n",
    "experiment_10 = pd.read_csv('exp10', index_col='time', parse_dates=True)\n",
    "\n",
    "# Combine all experiments into a list for easy processing\n",
    "experiments = [experiment_1, experiment_2, ..., experiment_10]\n",
    "\n",
    "\n",
    "\n",
    "# CONVERT DATAFRAMES TO PYTORCH \n",
    "import torch\n",
    "\n",
    "def df_to_tensor(df):\n",
    "    # Assuming 'df' is preprocessed and ready for conversion\n",
    "    # Convert df values to tensor\n",
    "    return torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# Convert each experiment to a tensor and store in a new list\n",
    "tensor_experiments = [df_to_tensor(df) for df in experiments]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PREPARE DATALOADERS\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Combine original features with extra parameters (one approach)\n",
    "combined_data = np.concatenate([tensor_experiments, extra_params], axis=-1)  # New shape [n_experiments, time_steps, n_features + n_extra_features]\n",
    "\n",
    "# Example targets for demonstration\n",
    "# In a real scenario, replace 'dummy_targets' with your actual targets\n",
    "dummy_targets = torch.rand(len(tensor_experiments), output_dim)  # Randomly generated targets\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dataset = TensorDataset(torch.stack(tensor_experiments), dummy_targets)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# BUILDING THE MODEL\n",
    "\n",
    "\n",
    "    # Tips \n",
    "        #1. Consider Multi-Task Learning (MTL):\n",
    "            # Multi-Task Learning is a learning paradigm in machine learning where multiple learning tasks are solved at the same time, \n",
    "            # while exploiting commonalities and differences across tasks. This approach can lead to improved learning efficiency and \n",
    "            # prediction accuracy for the task models, especially when the tasks are related. In deep learning, this often involves \n",
    "            # sharing layers between tasks, while having some task-specific layers towards the end of the mode \n",
    "            \n",
    "            # Imagine you have a dataset from your experiments, and you want to predict 4 main outputs, but you also have 3 other \n",
    "            # parameters that are somewhat related and you believe predicting them could help improve the performance of your main task.\n",
    "            \n",
    "            # In this example, MultiTaskLSTM is designed to make predictions for both the main task and an auxiliary task using shared \n",
    "            # LSTM layers for feature extraction, and separate fully connected layers for each task's specific output.\n",
    "            \n",
    "            \n",
    "        # 2. Consider Attention Mechanisms (Best to use Transformer ones)\n",
    "            # Attention mechanisms allow models to focus on different parts of the input for each output part, improving the ability of \n",
    "            # the model to capture dependencies, especially in sequences. In deep learning, attention mechanisms can dynamically weight \n",
    "            # the importance of input elements.\n",
    "            \n",
    "            # example: In this AttentionModel, an attention mechanism is applied to the output of an LSTM layer. The model computes \n",
    "            # attention weights for each element in the sequence, which are then used to create a weighted sum (context vector) \n",
    "            # representing the input sequence. This context vector is then fed into a fully connected layer to produce the final output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/household_power_consumption.txt\", sep=\";\",  parse_dates={'Datetime' : ['Date', 'Time']}, \n",
    "                          infer_datetime_format=True, low_memory=False, index_col='Datetime')\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_feat = df_well.drop('PP', axis=1) #replace 'PP' with the output label and change df to df_feat\n",
    "\n",
    "numerical_feature = [feature for feature in df.columns if df[feature].dtypes != 'O']\n",
    "discrete_feature=[feature for feature in numerical_feature if len(df[feature].unique())<25]\n",
    "continuous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]\n",
    "categorical_feature = [feature for feature in df.columns if feature not in numerical_feature]\n",
    "print(\"Numerical Features Count {}\".format(len(numerical_feature)))\n",
    "print(\"Discrete feature Count {}\".format(len(discrete_feature)))\n",
    "print(\"Continuous feature Count {}\".format(len(continuous_feature)))\n",
    "print(\"Categorical feature Count {}\".format(len(categorical_feature)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = ydata_profiling.ProfileReport(df) #replacing pandas_profiling with ydata_profiling\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "do_data_profiling(df, 'data_profiling.html') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML \n",
    "\n",
    "display(HTML(filename = 'data_profiling.html'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto EDA using dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale.show(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> EDA in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4a\"></a> <br>\n",
    "### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Basic formating (renaming cols, duplicates detection, datetime etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates \n",
    "df.drop_duplicates(inplace=True) \n",
    "\n",
    "#formating columns \n",
    "df.\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'price': 'selling_price', 'bedrooms': 'num_bedrooms'}, inplace=True)\n",
    "\n",
    "#replace non numeric columns\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> format datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "convert_timestamp(ts) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Remove unwanted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "df.drop(['id', 'date'], axis=1, inplace=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Handling Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True: \n",
    "            print(f\"there is {df[col].isnull().sum()} NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)  \n",
    "\n",
    "missing_value_analysis (df)\n",
    "view_NaN(df) \n",
    "\n",
    "\n",
    "# np.argwhere(df.isnull().values)   #to find the index with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values \n",
    "# df.fillna(df.mean(), inplace=True) \n",
    "\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df\n",
    "\n",
    "\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sklearn to handle missing values - (SimpleImputer, KNN-Imputer, Iterative Imputer, )\n",
    "\n",
    "\n",
    "#IterativeImputer: This function estimates missing values using a predictive model.\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "def impute_missing_values_iteratively(X): #or (X, Columns)\n",
    "    imputer = IterativeImputer(estimator = RandomForestRegressor())\n",
    "        \n",
    "    # select only the columns with missing values to be imputed\n",
    "    # X_cols = X[columns]\n",
    "    X_imputed = imputer.fit_transform(X) #or X_cols\n",
    "    return X_imputed\n",
    "\n",
    "impute_missing_values_iteratively(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize outliers\n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "visualize_outlier (df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Regression outliers using Cook's distance\n",
    "from yellowbrick.regressor import CooksDistance\n",
    "\n",
    "\n",
    "# Instantiate and fit the visualizer\n",
    "visualizer = CooksDistance()\n",
    "visualizer.fit(X, y)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform Data Distribution of relevant columns before determining whether or not they should be classified as outliers\n",
    "\n",
    "\n",
    "# Plotting the data distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df['max_power_bhp'], bins=20, color='blue')\n",
    "plt.xlabel('Max Power (bhp)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Data Distribution - Max Power (bhp)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_detect_arbitrary(data,col,upper_fence,lower_fence):\n",
    "    '''\n",
    "    identify outliers based on arbitrary boundaries passed to the function.\n",
    "    '''\n",
    "\n",
    "    para = (upper_fence, lower_fence)\n",
    "    tmp = pd.concat([data[col]>upper_fence,data[col]<lower_fence],axis=1)\n",
    "    outlier_index = tmp.any(axis=1)\n",
    "    print('Num of outlier detected:',outlier_index.value_counts()[1])\n",
    "    print('Proportion of outlier detected',outlier_index.value_counts()[1]/len(outlier_index))    \n",
    "    return outlier_index, para "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to handle outliers, use any of Tukey's test, Kernel density estimation, Z-score method, Mahalanobis distance method,\n",
    "#Isolation Forest model, EllipticEnvelope.\n",
    "\n",
    "\n",
    "# Tukey's test: This statistical method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the median and works well for univariate datasets with normal distributions.\n",
    "\n",
    "# Kernel density estimation: This non-parametric method estimates the probability density function of a dataset \n",
    "#     and identifies outliers as values with low probability density, making it suitable for non-normal datasets.\n",
    "\n",
    "# Z-score method: This simple method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the mean and is widely used for datasets with normal distributions.\n",
    "\n",
    "# Mahalanobis distance method: This multivariate method identifies outliers based on the distance of each point from the \n",
    "#     centroid of the dataset and is effective for datasets with multivariate normal distributions.\n",
    "\n",
    "# Isolation Forest model: This machine learning algorithm identifies outliers by isolating them into a separate tree \n",
    "#     structure, making it suitable for high-dimensional feature spaces with both linear and non-linear relationships \n",
    "#     between features.\n",
    "\n",
    "# EllipticEnvelope: This multivariate method identifies outliers by fitting an ellipse to the data and identifying \n",
    "#     points that are outside the ellipse, making it effective for datasets with multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4b\"></a> <br>\n",
    "### Data Transformation (scaling, encoding categorical data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Extracting features from Dates, Mixed Variables etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df.resample('H').mean() \n",
    "df_day = df.resample('D').mean() \n",
    "df_month = df.resample('M').mean() \n",
    "df_year = df.resample('Y').mean()\n",
    "\n",
    "\n",
    "df['hour'] = df.index.hour \n",
    "df['day'] = df.index.day \n",
    "df['weekday'] = df.index.day_name() \n",
    "df['month'] = df.index.month \n",
    "df['year'] = df.index.year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Categorical Variable Encoding (data transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding: replaces each category with a numerical label. This technique is suitable for data \n",
    "#     where the categories have an intrinsic order, such as \"low,\" \"medium,\" and \"high.\" Works well with linear models\n",
    "\n",
    "# Ordinal encoding: assigns a numerical value to each category based on their frequency. This technique is suitable \n",
    "#     for data where the categories do not have an intrinsic order, but where their frequency may be informative.\n",
    "#     Suitable for non-linear models\n",
    "\n",
    "# One-hot encoding: creates a binary variable for each category, indicating its presence or absence. \n",
    "#     This technique is suitable for data where the categories do not have an intrinsic order and the \n",
    "#     number of categories is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : array-like of shape (n_samples, n_features)\n",
    "        The input dataset.\n",
    "    test_size : float, optional\n",
    "        The proportion of the dataset to include in the test set.\n",
    "    val_size : float, optional\n",
    "        The proportion of the dataset to include in the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : array-like of shape (n_train_samples, n_features)\n",
    "        The training input samples.\n",
    "    X_val : array-like of shape (n_val_samples, n_features)\n",
    "        The validation input samples.\n",
    "    X_test : array-like of shape (n_test_samples, n_features)\n",
    "        The test input samples.\n",
    "    y_train : array-like of shape (n_train_samples,)\n",
    "        The target values (class labels) for the training input samples.\n",
    "    y_val : array-like of shape (n_val_samples,)\n",
    "        The target values (class labels) for the validation input samples.\n",
    "    y_test : array-like of shape (n_test_samples,)\n",
    "        The target values (class labels) for the test input samples.\n",
    "    \"\"\"\n",
    "    #reset index\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    # Split the dataset into train and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=test_size, random_state=42)\n",
    "\n",
    "    # Split the train set into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "    #show the shapes\n",
    "    print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler \n",
    "# # Set up the scaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Fit the scaler to the training set\n",
    "# scaler.fit(X_train) \n",
    "\n",
    "# # Transform the training and testing sets\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_val = scaler.transform(X_val) \n",
    "# X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Scaling (data transformation) - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if scaling only select features\n",
    "\n",
    "# columns_to_transform = ['year', 'km_driven', 'engine_CC', 'torque_Nm', \n",
    "#                         'seats', 'rpm', 'max_power_bhp', 'mileage_kmpl', 'vehicle_brand_target_encoded', \n",
    "#                         'vehicle_model_target_encoded']\n",
    "\n",
    "# # Create a StandardScaler object\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit the scaler on the selected columns\n",
    "# scaler.fit(X_train[columns_to_transform])\n",
    "\n",
    "# def scale_and_concat(df, scaler, columns_to_transform):\n",
    "#     \"\"\"\n",
    "#     Scales selected columns in a DataFrame using StandardScaler and concatenates them with the rest of the DataFrame.\n",
    "\n",
    "#     Args:\n",
    "#         df (pandas.DataFrame): The DataFrame containing the data.\n",
    "#         columns_to_transform (list): List of column names to be scaled.\n",
    "\n",
    "#     Returns:\n",
    "#         pandas.DataFrame: The concatenated DataFrame with scaled columns.\n",
    "#     \"\"\"\n",
    "#     # Create a copy of the original DataFrame\n",
    "#     df_concatenated = df.copy()\n",
    "\n",
    "#     # Scale the selected columns\n",
    "#     scaled_columns = scaler.transform(df[columns_to_transform])\n",
    "\n",
    "#     # Create a DataFrame with the scaled columns\n",
    "#     df_scaled = pd.DataFrame(scaled_columns, columns=columns_to_transform, index=df.index)\n",
    "\n",
    "#     # Concatenate the scaled columns with the rest of the DataFrame\n",
    "#     df_concatenated = pd.concat([df_concatenated.drop(columns=columns_to_transform, axis = 1), df_scaled], axis=1)\n",
    "\n",
    "#     return df_concatenated \n",
    "\n",
    "\n",
    "# X_train = scale_and_concat(X_train, scaler, columns_to_transform)\n",
    "# X_val = scale_and_concat(X_val, scaler, columns_to_transform) \n",
    "# X_test = scale_and_concat(X_test, scaler, columns_to_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the decision to scale or transform a variable should be based on \n",
    "    # its distribution, \n",
    "    # relationship with the target variable, and\n",
    "    # domain knowledge. \n",
    "# Scaling is not necessary if the variable follows a normal or approximately normal distribution; or if the variable has a linear \n",
    "# relationship with the target variable\n",
    "\n",
    "# Normalization - MinMax() - rescaling the values into a range of [0,1]. This process can be particularly useful when your features have \n",
    "# different scales and you want to standardize them without affecting the shape of the distributio\n",
    "\n",
    "# standardization - StandardScaler() - shifting the distribution of each attribute to have a mean of zero and a standard deviation of one \n",
    "# (unit variance). It is useful in cases where the data follows a Gaussian distribution\n",
    "\n",
    "\n",
    "#if scaling all the features \n",
    "def standardize_data(X_train, X_val, X_test): \n",
    "    \"\"\"\n",
    "    Standardizes the training and testing data using the mean and standard deviation\n",
    "    learned from the training set.\n",
    "    \n",
    "    Args:\n",
    "    - X_train: numpy array or pandas dataframe, training data\n",
    "    - X_test: numpy array or pandas dataframe, testing data\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled: numpy array or pandas dataframe, standardized training data\n",
    "    - X_test_scaled: numpy array or pandas dataframe, standardized testing data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    # Set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the training set\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    # Transform the training and testing sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return scaler, X_train_scaled, X_val_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "from typing import List \n",
    "def to_DataFrame(Dataset, columns:List) -> DataFrame: \n",
    "    df = pd.DataFrame(Dataset, columns=columns)\n",
    "    return df \n",
    "\n",
    "\n",
    "scaler, X_train, X_val, X_test = standardize_data(X_test, X_test) \n",
    "X_train = to_DataFrame(X_train, columns= continuous_feature)\n",
    "X_val = to_DataFrame(X_val, columns= continuous_feature)\n",
    "X_test = to_DataFrame(X_test, columns= continuous_feature) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Variable Transformation - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the decision to scale or transform a variable should be based on its distribution, relationship with the \n",
    "# target variable, and domain knowledge. Scaling is not necessary if the variable follows a normal or approximately \n",
    "# normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable transformation involves transforming the values of variables to make them more suitable for analysis\n",
    "#the idea is to make the variables normally/gaussian distributed. Hence, \n",
    "\n",
    "#first step is to assess normality using a histogram or QQ-plot (to explore the variable distribution)\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.title(f\"Histogram of {variable}\")\n",
    "\n",
    "    # q-q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot of {variable}\")\n",
    "\n",
    "    # check for skewness\n",
    "    skewness = df[variable].skew()\n",
    "    if skewness > 0:\n",
    "        skew_type = \"positively skewed\"\n",
    "    elif skewness < 0:\n",
    "        skew_type = \"negatively skewed\"\n",
    "    else:\n",
    "        skew_type = \"approximately symmetric\"\n",
    "        \n",
    "    # print message indicating skewness type\n",
    "    print(f\"The variable {variable} is {skew_type} (skewness = {skewness:.2f})\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# # Check function output\n",
    "# diagnostic_plots(X, \"MedInc\")\n",
    "\n",
    "\n",
    "#use this to make diagnostics plot for all variables\n",
    "for feature in continuous_feature:\n",
    "    print(feature)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[feature], dist=\"norm\", plot=plt) \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qq_plots(df, variable):     \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.show()\n",
    "\n",
    "for feature in continuous_feature:\n",
    "    print(feature)\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[feature].hist()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[feature], dist=\"norm\", plot=plt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the variables are NOT normally distributed, we then transform it. It is necessary to test several variable \n",
    "# transformation methods, and choose the best for that feature. One variable transformation method is log_transform\n",
    "\n",
    "#NB: if data is positively skewed (right skewed), use (logarithmic, reciprocal, or square root transformation)\n",
    "    #if data is negatively skewed (left skewed), use (Box-Cox or Yeo-Johnson transformations)\n",
    "#log transform \n",
    "def log_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the natural logarithm function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_log = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_log\n",
    "\n",
    "#perform transformation on the variables that are not normally distributed\n",
    "X_train = log_transform(X_train, columns = ['Vp', 'Caliper'])\n",
    "X_val = log_transform(X_val, columns = ['Vp', 'Caliper'])\n",
    "X_test = log_transform(X_test, columns = ['Vp', 'Caliper']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df_log, columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Discretization (data transformation - apply to train, and then to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization in machine learning is the process of transforming continuous variables into discrete or \n",
    "# categorical variables. This process involves dividing the range of a continuous variable into a finite number of \n",
    "# intervals or bins, and then assigning each observation to a particular bin based on the value of the continuous \n",
    "# variable. \n",
    "\n",
    "#Discretization approaches: equal width, equal frequency, K means, Decision Trees\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4c\"></a> <br>\n",
    "### Handling Imbalanced Data and Biases (Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to train, and then to test \n",
    "\n",
    "#Imbalanced data refers to a situation where the number of observations in one class or category is much larger \n",
    "# or smaller than the number of observations in other classes or categories. Imbalanced data can pose challenges \n",
    "# in machine learning because it can lead to biased models that perform poorly on the minority class.\n",
    "\n",
    "#we typically look at the 'target data' when checking for imbalanced data. However, it is also important \n",
    "# to consider the features\n",
    "\n",
    "def check_imbalance(dataset, columns=None, threshold=10):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and one or more columns as input and returns True if any of the specified columns\n",
    "    are imbalanced, False otherwise. A column is considered imbalanced if the percentage of the minority class is less\n",
    "    than the specified threshold.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, use all columns except for the last one as the features\n",
    "    if columns is None:\n",
    "        features = dataset.iloc[:, :-1]\n",
    "        columns = features.columns\n",
    "    \n",
    "    # Check the imbalance of each specified column\n",
    "    for col in columns:\n",
    "        # Get the counts of each class in the column\n",
    "        class_counts = dataset[col].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each class in the column\n",
    "        class_percentages = class_counts / len(dataset) * 100\n",
    "\n",
    "        # Plot the class percentages\n",
    "        plt.bar(class_counts.index, class_percentages)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the column is imbalanced\n",
    "        minority_class = class_counts.index[-1]\n",
    "        minority_class_percentage = class_percentages.iloc[-1]\n",
    "        if minority_class_percentage < threshold:\n",
    "            print(f'{col} is imbalanced. Minority class: {minority_class}, Percentage: {minority_class_percentage:.2f}%')\n",
    "            return True\n",
    "\n",
    "    # If none of the specified columns are imbalanced, return False\n",
    "    print('No imbalance found.')\n",
    "    return False\n",
    "\n",
    "check_imbalance(df, columns=['target'], threshold=10) \n",
    "\n",
    "\n",
    "#OR\n",
    "\n",
    "from yellowbrick.target import ClassBalance \n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance(labels=[\"draw\", \"loss\", \"win\"])\n",
    "visualizer.fit(y_train, y_test)        # Fit the data to the visualizer (you can also use visualizer.fit(y))\n",
    "visualizer.show()                       # Finalize and render the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is imbalance, you can handle it by over-sampling or under-sampling the dataset\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def handle_imbalanced_data(X, y, strategy='over-sampling'):\n",
    "    \"\"\"\n",
    "    Handle imbalanced data using imblearn library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array-like of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y: array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    strategy: str, default='over-sampling'\n",
    "        The strategy to use for handling imbalanced data. Possible values are\n",
    "        'over-sampling' and 'under-sampling'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled: array-like of shape (n_samples_new, n_features)\n",
    "        The resampled input data.\n",
    "    y_resampled: array-like of shape (n_samples_new,)\n",
    "        The resampled target values.\n",
    "    \"\"\"\n",
    "    if strategy == 'over-sampling':\n",
    "        # Initialize the RandomOverSampler object\n",
    "        ros = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    elif strategy == 'under-sampling':\n",
    "        # Initialize the RandomUnderSampler object\n",
    "        rus = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Possible values are 'over-sampling' and 'under-sampling'.\")\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4d\"></a> <br>\n",
    "### Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step involves reducing the dimensionality of the data by selecting a subset of the original features or \n",
    "# by transforming the data using techniques like Principal Component Analysis (PCA).\n",
    "\n",
    "#Feature selection\n",
    "    # Removing features with low variance (VarianceThreshold)\n",
    "    # Univariate Feature Selection (SelectKBest, SelectPercentile, GenericUnivariateSelect)\n",
    "    # Recursive Feature Elimination (RFE, RFECV)        RFECV - RFE cross validation\n",
    "    # Feature selection using SelectFromModel (SelectFromModel) - use L1-based (Lasso, Ridge, ElasticNet) or Tree-based\n",
    "    # Sequential Feature Selection (SequentialFeatureSelector) - SFS can be either forward or backward\n",
    "\n",
    "\n",
    "#Feature extraction\n",
    "    # Principal Component Analysis (PCA)\n",
    "    # Independent Component Analysis (ICA)\n",
    "    # t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## Selecting and Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you choose a ML technique to train the model, consider if your ML technique is resistant to the following:\n",
    "#     Missing data\n",
    "#     Data imbalance\n",
    "#     Feature Scaling\n",
    "#     Categorical Data\n",
    "#     Outliers\n",
    "#     Dimensionality (refers to the number of features or variables in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use AutoML (check for PyCaret and Auto SKlearn - see AutoML file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> AutoML using Pycaret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(df, columns):\n",
    "    df = df.reset_index()\n",
    "    df = df.drop(columns = [columns], axis = 1)\n",
    "    return df\n",
    "\n",
    "y_train_pycaret = reset_index(y_train)\n",
    "y_val_pycaret = reset_index(y_val) \n",
    "y_test_pycaret = reset_index(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pycaret\n",
    "from pycaret.classification import * \n",
    "from pycaret.classification import setup, compare_models, create_model, tune_model, plot_model, evaluate_model, save_model\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True) \n",
    "X['target'] = y \n",
    "# Initialize classification setup \n",
    "# clf1 = setup(data=X, target='target', train_size = 0.8, \n",
    "#              preprocess = True, polynomial_features = True, \n",
    "#              polynomial_degree = 2, fix_imbalance = True,\n",
    "#              fix_imbalance_method = 'SMOTE', feature_selection = True,\n",
    "#              feature_selection_method = ' ', feature_selection_estimator = ,\n",
    "#              n_features_to_select = 0.2) \n",
    "\n",
    "\n",
    "clf1 = setup(data=X, target='target', train_size = 0.8, session_id = 123)\n",
    "# all_models = models()   #use this to visualize a table of models available in the model library.\n",
    "\n",
    "# Compare models \n",
    "compare_results = compare_models(n_select=5)    #the best 5 models will be highlighted\n",
    "\n",
    "compare_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for model in compare_results:\n",
    "    evaluate_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = create_model('knn')     #change knn to any of the top 5 models from above\n",
    "            # from pycaret.regression import models     #change to classification when needed, then do: exp = setup(X, y)\n",
    "            # regression_models = models() # Get all regression model estimators  \n",
    "            # print(regression_models) # Display the list of model names\n",
    "\n",
    "\n",
    "# # Tune the model\n",
    "tuned_model = tune_model(model)\n",
    "\n",
    "# # Evaluate the model\n",
    "evaluate_model(tuned_model)\n",
    "\n",
    "# # Fit the model\n",
    "final_model = tune_model(tuned_model)\n",
    "\n",
    "# Save the final model in the \"ML\" folder\n",
    "model_path = 'models/pycaret_ExtraTreesRegressor_r2'\n",
    "save_model(final_model, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Then select the preferred model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest classifier on the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# # Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[1,1], random_state=1)\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = roc_auc_score(train_y, train_preds)\n",
    "        test_loss = roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "y_pred, test_loss, test_preds2, clf = runRF(\n",
    "                                            X_train,\n",
    "                                            y_train,\n",
    "                                            X_val,\n",
    "                                            y_val=None,\n",
    "                                            test_X2=None,\n",
    "                                            rounds=100,\n",
    "                                            depth=20,\n",
    "                                            leaf=10,\n",
    "                                            feat=0.2,\n",
    "                                            min_data_split_val=2,\n",
    "                                            seed_val=0,\n",
    "                                            job=-1,\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification performance two-class report\n",
    "\n",
    "\n",
    "y_pred_proba,ACC,PC,RC,FS,AP,roc_auc,gini = print_classification_performance2class_report(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#classification performance multi-class report\n",
    "\n",
    "\n",
    "y_pred, ACC, PC, RC, FS, roc_auc = print_classification_performance_multiclass_report(model, X_test, y_test)\n",
    "\n",
    "# ROC AUC is calculated using a One-vs-Rest approach. This involves treating each class as a binary classification against all other classes.\n",
    "# Precision, recall, and F1 score are calculated as a weighted average to account for class imbalances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression performance report \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "mse, rmse, r2, y_pred = print_regression_performance_report(model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "    # Error = Actual value - Predicted value\n",
    "\n",
    "    # MSE (Mean Square Error)\n",
    "        # The square of the error over all samples is called Mean Squarred Error(MSE).\n",
    "        # MSE = SQUARE(Actual value - Predicted value)/Number of Samples\n",
    "    #RMSE (Root Mean Square Error)\n",
    "    # MAE (Mean Absolute Error)\n",
    "        # MAE = ABSOLUTE (Actual value - Predicted Value)\n",
    "\n",
    "\n",
    "#Classification\n",
    "    #Accuracy: (TP + TN / {TP + FP + TN + FN})\n",
    "        # Measures the proportion of correct predictions among the total number of cases processed.\n",
    "        # Useful for balanced datasets but can be misleading in the presence of class imbalances.\n",
    "        \n",
    "    #Precision: (TP / {TP + FP})\n",
    "        # Indicates the proportion of positive identifications that were actually correct.\n",
    "        # Particularly important in scenarios where false positives are a significant concern.\n",
    "        \n",
    "    #Recall (Sensitivity): (TP / {TP + FN})\n",
    "        # Measures the proportion of actual positives that were correctly identified.\n",
    "        # Critical in situations where missing a positive is significantly worse than falsely identifying a negative.\n",
    "    \n",
    "    # Specitivity: (TN/ {TN+FP})\n",
    "        # Specificity measures the proportion of actual negatives that are correctly identified as such.\n",
    "        # High specificity implies the model is effective in ruling out the condition when its not present\n",
    "        \n",
    "    #F1 score: (2 * {[PC*RC] / [PC+RC]} )\n",
    "        # A harmonic mean of precision and recall. Provides a balance between precision and recall in one number.\n",
    "        # Particularly useful when you need to compare two or more models or when theres an uneven class distribution.\n",
    "        \n",
    "    #AUC-ROC curve\n",
    "        # It plots the True Positive Rate (Recall) against the False Positive Rate (1 - Specificity) at various threshold settings. \n",
    "        # The Area Under the Curve (AUC) quantifies the overall ability of the test to discriminate between positive and negative classes\n",
    "        # Higher the AUC, the better the model is at distinguishing between positive and negative classes\n",
    "\n",
    "    # Precision-Recall Curve:\n",
    "        # Shows the tradeoff between precision and recall for different threshold values.\n",
    "        # A high area under the curve represents both high recall and high precision.\n",
    "        # More informative than ROC curves in case of highly imbalanced datasets.\n",
    "        \n",
    "    # Confusion Matrix:    \n",
    "        # True Positives(TP): Number of samples that are correctly classified as positive, and their actual label is positive.\n",
    "        # False Positives (FP): Number of samples that are incorrectly classified as positive, when in fact their actual label \n",
    "            # is negative.\n",
    "        # True Negatives (TN): Number of samples that are correctly classified as negative, and their actual label is negative.\n",
    "        # False Negatives (FN): Number of samples that are incorrectly classified as negative, when in fact their actual label \n",
    "            # is positive.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "## Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Class Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "# classes = [\"apple\", \"kiwi\", \"pear\", \"banana\", \"orange\"]\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassPredictionError(model, classes=classes)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Learning Curve (Access the Bias and Variance) - Model Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Bias` - error in the training data\n",
    "\n",
    "- `Variance` -> difference in the errors between the train and test data. (i.e.,  it examines how the model's performance varies between the training data and unseen data (like a validation set).  If the model performs well on the training data but poorly on the validation/test data, it suggests high variance)\n",
    "\n",
    "\n",
    "`High Bias (will also have High Variance) -> Underfitting:` The model is too simple and doesn't capture the complexities of the data well, leading to poor performance on both training and testing datasets.\n",
    "\n",
    "`Low Bias + High Variance -> Overfitting:` The model is too complex, fitting too closely to the training data, including its noise and outliers. It performs well on training data but poorly on unseen data.\n",
    "\n",
    "`Low Bias + Low Variance -> Ideal Model:` This is the desired outcome. The model accurately captures the underlying patterns in the data (low bias) and generalizes well to unseen data (low variance).\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"learning-curves.png\" alt=\"Example Image\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check Bias and Variance using Learnng Curve \n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10) # Define the training set sizes to plot the learning curve\n",
    "\n",
    "def cv_learning_curve(model, X, y, cv, train_sizes):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "                                                #scoring parameter -  #https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter \n",
    "    train_mean = np.mean(-train_scores, axis=1)\n",
    "    train_std = np.std(-train_scores, axis=1)\n",
    "    test_mean = np.mean(-test_scores, axis=1)\n",
    "    test_std = np.std(-test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Error')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Validation Error')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_sizes, train_mean, train_std, test_mean, test_std\n",
    "\n",
    "\n",
    "\n",
    "# Generate learning curve plot\n",
    "train_sizes, train_mean, train_std, test_mean, test_std = cv_learning_curve(model, X, y, cv, train_sizes)\n",
    "\n",
    "\n",
    "# To fix high bias (underfitting):\n",
    "    # get additional features or increasing the size of the model \n",
    "    # Adding polynomial features is a form of feature engineering that can increase the complexity of the model\n",
    "    # decrease the regularization parameter (lambda) to allow the model's learning algorithm to fit the data more flexibly, \n",
    "        # thereby potentially reducing bias\n",
    "\n",
    "# To fix high variance (overfitting):\n",
    "    # Obtaining more training samples can help the model generalize better.\n",
    "    # Simplifying the model by reducing the number of features (feature selection) can prevent the model from fitting noise in the \n",
    "        # training data.\n",
    "    # Increasing the regularization parameter (lambda) adds a penalty to the model complexity, which can help in preventing overfitting.\n",
    "\n",
    "\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "visualizer = LearningCurve(\n",
    "    model, cv=cv, scoring='f1_weighted', train_sizes=sizes, n_jobs=4\n",
    "        )\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, cross_val_score, KFold, train_test_split\n",
    "from yellowbrick.model_selection import CVScores #visualizing the cross validation scores\n",
    "\n",
    "#check Bias and Variance using Cross Validation\n",
    "\n",
    "cv = 5  #or # Create a cross-validation object: \n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "def cv_bias_variance(model, X, y, cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')   \n",
    "    train_error = -scores.mean()\n",
    "    val_error = -scores.std()\n",
    "    return train_error, val_error, scores\n",
    "\n",
    "# options to replace scoring:\n",
    "#     regression: r2, neg_mean_absolute_error, explained_variance, neg_root_mean_squared_error, etc.\n",
    "#     classification: accuracy, f1, roc_auc, precision, recall, etc.  \n",
    "\n",
    "\n",
    "# Calculate the mean training and validation error scores\n",
    "train_error, val_error, scores = cv_bias_variance(model, X, y, cv)\n",
    "print(\"Mean training error:\", train_error)\n",
    "print(\"Mean validation error:\", val_error)\n",
    "\n",
    "\n",
    "visualizer = CVScores(model, cv=cv, scoring='r2')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot Validation Curves (to analyse the impact of each Hyperparameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to analyze the impact of varying hyperparameter values on the model's performance using Validation curve\n",
    "#the validation curve is useful for hyperparameter tuning, while the learning curve is used to assess bias and variance.\n",
    "\n",
    "# Using validation curves is a fundamental aspect of machine learning model tuning, as it provides valuable insights into how \n",
    "# hyperparameters influence model performance, aiding in the selection of the most appropriate model settings.\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "\n",
    "\n",
    "#the hyperparameter used here is 'max_depth'. A hyperparameter for the model ExtraTreeRegressor\n",
    "\n",
    "plot_validation_curve(model, X_train, y_train, param_name=\"max_depth\", cv=5, scoring=\"r2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import ValidationCurve \n",
    "\n",
    "# cv = StratifiedKFold(4)\n",
    "viz = ValidationCurve(\n",
    "    DecisionTreeRegressor(), param_name=\"max_depth\",\n",
    "    param_range=np.arange(1, 11), cv=10, scoring=\"r2\"\n",
    ")\n",
    "\n",
    "# Fit and show the visualizer\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Analyze Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the errors are normally distributed around zero, it may indicate that the model is making unbiased predictions. \n",
    "# If there is a pattern or trend in the errors, it may suggest that the model has systematic biases or is making \n",
    "# consistent errors in certain regions of the input space\n",
    "\n",
    "\n",
    "analyze_error_distribution(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Error Analysis - Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error analysis is the process of analyzing the errors made by a machine learning model and identifying the patterns \n",
    "# or trends that may be causing the errors. The goal of error analysis is to gain insight into the behavior of the \n",
    "# model and identify areas for improvement. \n",
    "\n",
    "# The steps involved in error analysis:\n",
    "    # 1. Collect error data - Gather instances where the model made incorrect predictions.\n",
    "    # 2. Categorize errors - Classify errors into meaningful categories.\n",
    "    # 3. Identify patterns - Look for commonalities or trends among the errors.\n",
    "    # 4. Analyze causes - Investigate potential reasons behind these patterns.\n",
    "    # 5. Prioritize fixes - Decide which errors to address first based on their impact.\n",
    "\n",
    "    \n",
    "# Based on the insights gained from the error analysis, you can perform the following.\n",
    "\n",
    "# False negatives: \n",
    "    # To fix this issue, you may consider the following:\n",
    "        # Increase the weight of the features that are more indicative of churn for low-usage customers, \n",
    "            # such as frequency of usage or specific product usage. (adjust the model parameters)\n",
    "        # Add new features that may be predictive of churn, such as customer sentiment or customer service interactions.\n",
    "        # Use a different model architecture that is better suited for handling imbalanced data, such as a decision tree \n",
    "            # or ensemble model. \n",
    "\n",
    "\n",
    "# False positives:\n",
    "    # To fix this issue, you may consider the following:\n",
    "        # Decrease the weight of features that are causing false positives, such as age or income, if they are not as \n",
    "            # indicative of churn for low-usage customers. (adjust the model parameters)\n",
    "        # Remove features that are causing false positives altogether, if they are not providing significant value to the \n",
    "            # model.\n",
    "        # Increase the size of the training dataset to capture a more representative sample of customers who do not churn, \n",
    "            # which may help the model learn more accurately which customers are likely to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix to visualize false positives and false negatives\n",
    "    #By default, scikit-learn will assume that the \"positive\" class is the last label (or highest label value) \n",
    "    # in the list of labels. [0, 1] where 1 is Positive and is the class_of_interest.\n",
    "\n",
    "\n",
    "class_names = [0, 1] #or iris().target_names #this is an example and should be edited. [0, 1] for binary classification\n",
    "class_of_interest = 1 #this selects a specific class of interest other than 1 or the highest value. \n",
    "                        #always select the highest one because that is what Scikit learn uses. \n",
    "\n",
    "def false_positives(X_test, y_true, y_pred, classes):\n",
    "    \"\"\" \n",
    "    This function identifies and plots the false positives in a classification problem. \n",
    "    \"\"\" \n",
    "    fp_indices = np.where((y_true != class_of_interest) & (y_pred == class_of_interest))[0] \n",
    "    fp_features = X_test[fp_indices] # assuming X_test is a numpy array of input data \n",
    "    # fp_features = X_test.iloc[fp_indices]\n",
    "    fp_labels = y_pred[fp_indices] # assuming y_pred is a numpy array of predicted labels \n",
    "    # fp_labels = pd.Series(y_pred).iloc[fp_indices]\n",
    "\n",
    "    print(\"False positives: \", len(fp_indices))\n",
    "    return fp_features, fp_labels\n",
    "\n",
    "\n",
    "#false negatives \n",
    "def false_negatives(X_test, y_true, y_pred, classes):\n",
    "    \"\"\" \n",
    "    This function identifies and plots the false negatives in a classification problem. \n",
    "    \"\"\" \n",
    "    fn_indices = np.where((y_true == class_of_interest) & (y_pred != class_of_interest))[0] \n",
    "    fn_features = X_test[fn_indices] # assuming X_test is a numpy array of input data\n",
    "    # fn_features = X_test.iloc[fn_indices] \n",
    "    fn_labels = y_pred[fn_indices] # assuming y_pred is a numpy array of predicted labels \n",
    "    # fn_labels = pd.Series(y_pred).iloc[fn_indices]\n",
    "\n",
    "    print(\"False negatives: \", len(fn_indices))\n",
    "    return fn_features, fn_labels\n",
    "\n",
    "\n",
    "# Plot the confusion matrix to evaluate the performance of the model\n",
    "plot_confusion_matrix(y_test, y_pred, classes=classes,\n",
    "                      title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))\n",
    "\n",
    "# Identify and plot the false positives\n",
    "X_fp, y_fp = false_positives(X_test, y_test, y_pred, class_names)\n",
    "\n",
    "# Identify and plot the false negatives\n",
    "X_fn, y_fn = false_negatives(X_test, y_test, y_pred, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "## Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the most important features in the model\n",
    "\n",
    "#depending on the results from the bias and variance tests, there may be need to assess which features\n",
    "# are the most important in the ML model\n",
    "\n",
    "\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "feature_importance(model,X_train)  \n",
    "\n",
    "\n",
    "\n",
    "#OR\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "viz = FeatureImportances(model, labels=labels, relative=False)\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualize Model Performance during Feature Selection/Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.model_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold \n",
    "\n",
    "\n",
    "cv = StratifiedKFold(5)\n",
    "visualizer = RFECV(RandomForestClassifier(), cv=cv, scoring='f1_weighted')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Selection and Extraction - (Fix High Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addressing High Variance in Models through Feature Selection/Extraction\n",
    "\n",
    "# Overview\n",
    "    # Goal: Improve model generalization by reducing complexity or enhancing informative representations.\n",
    "    # Application: Apply techniques to training data, then to test data.\n",
    "    \n",
    "# Initial Steps\n",
    "    # Assess feature importance.\n",
    "    # Options:\n",
    "        # Add or remove features based on their importance.\n",
    "        # Consider creating polynomial features for more complex relationships.\n",
    "\n",
    "# Feature Selection/Extraction Strategies\n",
    "\n",
    "    # Step 1: Manual Feature Adjustment\n",
    "        # Add or remove features based on domain knowledge or preliminary analysis.\n",
    "        \n",
    "    # Step 2: Automated Techniques (start with feature selection)\n",
    "    \n",
    "        # Feature Selection Methods:\n",
    "            # VarianceThreshold: Remove features with low variance.\n",
    "            # Univariate Selection: Use methods like SelectKBest, SelectPercentile, or GenericUnivariateSelect.\n",
    "            # Recursive Feature Elimination: RFE or RFECV (with cross-validation).\n",
    "            # Model-Based Selection: Use SelectFromModel with L1-based (Lasso, Ridge, ElasticNet) or tree-based methods.\n",
    "            # Sequential Feature Selector: Forward or backward selection (SFS).\n",
    "            \n",
    "        # Feature Extraction Methods:\n",
    "            # Principal Component Analysis (PCA).\n",
    "            # Independent Component Analysis (ICA).\n",
    "            # t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "        \n",
    "        # Common Choices: PCA for extraction and SelectKBest for selection.\n",
    "\n",
    "# Workflow for Model Improvement \n",
    "    # Build and test a model with normal data.\n",
    "    # If accuracy is not satisfactory, perform feature selection and retest.\n",
    "        # Optionally, add polynomial features to the selected features and retest.\n",
    "    # If needed, proceed to feature extraction and retest.\n",
    "        # Again, consider adding polynomial features and retesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adding Polynomial Features (Fix High Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Polynomial Features (to Fix High Bias) - do this only if there is high variance\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df_polynomial = add_polynomial_features_sklearn(df, degree, columns=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Performing Regularization (fix High Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to fix High Bias or Variance is to perform regularization on the model.\n",
    "#this would involve increasing or decreasing the regularization parameter (lambda) to fix high variance or bias\n",
    "\n",
    "\n",
    "# By tuning the hyperparameters of the model using cross-validation, \n",
    "# we would have effectively applied regularization to the model, which can help to reduce overfitting and improve \n",
    "# its generalization performance.\n",
    "\n",
    "#Hence the next step is MODEL OPTIMIZATION. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "## Model Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto Hyperparameter Optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import optuna\n",
    "\n",
    "SVC().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the logging level for Optuna to WARNING\n",
    "# logging.getLogger('optuna').setLevel(logging.WARNING) \n",
    "\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters' search space\n",
    "    C = trial.suggest_loguniform('C', 1e-4, 1e4)\n",
    "    kernel = trial.suggest_categorical('kernel', ['rbf', 'poly'])\n",
    "    gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "    degree = trial.suggest_int('degree', 1, 5) if kernel == 'poly' else 3  # degree is only used for 'poly' kernel\n",
    "\n",
    "    # Create the SVM model\n",
    "    model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, probability=True)\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=10,random_state=11,shuffle=True) \n",
    "    \n",
    "    # Perform cross-validation and compute the average AUC score\n",
    "    scores = cross_val_score(model, train_data, train_labels, scoring='roc_auc', n_jobs=-1, cv=cv)\n",
    "    avg_auc_test = np.mean(scores)\n",
    "\n",
    "    return avg_auc_test\n",
    "\n",
    "# Create and run the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_hyperparams = study.best_trial.params\n",
    "# print('Best Hyperparameters:', best_hyperparams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters obtained from Optuna\n",
    "best_C_svm = best_hyperparams['C']\n",
    "best_kernel = best_hyperparams['kernel']\n",
    "best_gamma = best_hyperparams['gamma']\n",
    "best_degree = best_hyperparams['degree'] \n",
    "\n",
    "\n",
    "print('Best C (svm): {}'.format(best_C_svm))\n",
    "print('Best kernel: {}'.format(best_kernel))\n",
    "print('Best gamma: {}'.format(best_gamma))\n",
    "print('Best degree: {}'.format(best_degree))\n",
    "\n",
    "tuned_svm_model = SVC(C=best_C_svm, kernel=best_kernel, gamma=best_gamma, degree = best_degree, probability=True)\n",
    "\n",
    "tuned_svm_model.fit(train_data, train_labels)\n",
    "\n",
    "dump(tuned_svm_model, 'models/tuned_model/tuned_support_vector.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred,ACC,PC,RC,FS,AP,roc_auc,gini = print_classification_performance2class_report(tuned_svm_model,valid_data,valid_labels) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "## Model Deployment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Save the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a trained scikit-learn model to disk using joblib.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        joblib.dump(model, filename)\n",
    "        # joblib.dump(pipeline, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {filename}: {e}\")\n",
    "\n",
    "save_model(model, 'model.joblib')\n",
    "\n",
    "\n",
    "# model = joblib.load() # to load the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Build a Data pre-processing pipeline for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "#Learn to use the following: Pipeline, FeatureUnion, FunctionTransformer, ColumnTransformer\n",
    "# set_config(display='diagram')   #to visualize pipeline    \n",
    "\n",
    "# Load the pre-trained model\n",
    "model = joblib.load('breast_cancer.joblib')\n",
    "\n",
    "\n",
    "# Define the preprocessing pipeline\n",
    "def preprocessing_data (user_inputs:float) -> np.ndarray:\n",
    "\n",
    "    # Create a DataFrame from the user inputs\n",
    "    X_new = pd.DataFrame(user_inputs, columns=X.columns,index=[0])\n",
    "\n",
    "    # Define transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=2)),\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    ])\n",
    "\n",
    "    # Define pipeline using FeatureUnion\n",
    "    preprocessor = FeatureUnion(transformer_list=[\n",
    "        ('numeric_transformer', numeric_transformer),\n",
    "        ('categorical_transformer', categorical_transformer),\n",
    "    ])\n",
    "\n",
    "    # Fit and transform preprocessor on test data\n",
    "    X_test = preprocessor.fit_transform(X_new)\n",
    "\n",
    "    return X_test \n",
    "\n",
    "# #use this to visualize the pipelines as a diagram \n",
    "# from sklearn import set_config\n",
    "# set_config(display='diagram')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Proof of Concept (POC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML Interactive Interface using Gradio\n",
    "\n",
    "#before deploying your model to production, you may need to show a POC (proof of concept) i.e a prototype\n",
    "#use Gradio library for interfaces for your ML model \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the model here\n",
    "\n",
    "# https://dashboard.render.com/\n",
    "# sign-in with Github\n",
    "#or use Heroku, GCP, AWS, AZUre, IBM Watson etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[1,1], random_state=1)\n",
    "feature_names = ['Feature_{}'.format(i) for i in range(X.shape[1])] # Create feature names\n",
    "\n",
    "\n",
    "#you can use both RandomGridSearch and GridSearch to optimize a model. Use RandomGridSearch first, then GridSearch\n",
    "rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "rfc_random = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), random_grid, verbose=2, cv=3, refit=True)\n",
    "        # estimator: This is the machine learning model or estimator that you want to optimize using hyperparameter tuning\n",
    "        # param_distributions: This parameter specifies the hyperparameter space to be searched during the random search\n",
    "        # n_iter: This specifies the number of random combinations of hyperparameter values to try during the search.\n",
    "        # cv: This parameter determines the number of folds in the cross-validation process.\n",
    "        # verbose: This controls the verbosity of the output during the hyperparameter search. \n",
    "        #     A higher value, such as verbose=2, means more detailed output will be displayed during the search.\n",
    "        # random_state: This parameter sets the random seed for reproducibility\n",
    "        # n_jobs: This specifies the number of CPU cores to use for parallelization during the hyperparameter search. \n",
    "        #     A value of -1 (n_jobs=-1) means that all available CPU cores will be used.\n",
    "        # scoring: This parameter specifies the scoring metric used to evaluate the performance of the model \n",
    "        #     with different hyperparameter values. It can be set to a string representing a scoring metric, \n",
    "        #     such as 'accuracy', 'precision', 'recall', 'f1', etc., or an object of a custom scoring function\n",
    "        # refit: This parameter determines whether the best hyperparameters found during the search should be used to \n",
    "        #     refit the model on the entire dataset after the search is complete.\n",
    "        \n",
    "# Cross-validation techniques (three of the most common):\n",
    "#     K-fold Cross-Validation: It divides the data into k equally sized folds and performs training and testing on \n",
    "#         k iterations. It is widely used due to its simplicity and provides a good balance between bias and variance.\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     Stratified K-fold Cross-Validation: It is similar to K-fold cross-validation, but it ensures that each fold has an \n",
    "#         approximately equal distribution of target classes, making it suitable for imbalanced datasets.\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     Time Series Cross-Validation: It is used for time series data, where the order of data points matters. \n",
    "#         It involves using a sliding time window to create overlapping train and test sets, taking into account \n",
    "#         temporal dependencies.\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#      Leave-One-Out Cross-Validation (LOOCV): It is a special case of K-fold cross-validation where k is set to the \n",
    "#         total number of samples, resulting in each sample being used as a test set once. It is computationally expensive \n",
    "#         but can be useful for small datasets.\n",
    "        from sklearn.model_selection import LeaveOneOut\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
